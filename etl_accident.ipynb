{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46003352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration charg√©e et moteur de base de donn√©es cr√©√©.\n"
     ]
    }
   ],
   "source": [
    "# Installer les biblioth√®ques\n",
    "#pip install jupyterlab pandas requests sqlalchemy psycopg2-binary geopandas\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sqlalchemy import create_engine\n",
    "from shapely.geometry import Point\n",
    "import io\n",
    "\n",
    "# --- CONSTANTES ---\n",
    "# URL de l'API Opendatasoft\n",
    "# Nous limitons √† 10000 enregistrements pour cet exemple. Retirez le param√®tre &rows=10000 pour tout r√©cup√©rer.\n",
    "#API_URL = \"https://public.opendatasoft.com/explore/assets/accidents-corporels-de-la-circulation-millesime/\"\n",
    "\n",
    "\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# API\n",
    "API_URL = \"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents-corporels-de-la-circulation-millesime/records\"\n",
    "\n",
    "# Nombre d'enregistrements √† r√©cup√©rer par appel (l'API est limit√©e √† 100)\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Base de donn√©es (identifiants de votre docker-compose.yml)\n",
    "DB_USER = \"user_securite_routiere\"\n",
    "DB_PASSWORD = \"password123\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"securite_routiere_db\"\n",
    "DATABASE_URL = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "# Cr√©e le moteur de connexion √† la base de donn√©es\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "print(\"Configuration charg√©e et moteur de base de donn√©es cr√©√©.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24352d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration charg√©e. Pr√™t pour l'extraction.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "# URL de l'API pour le jeu de donn√©es des accidents corporels\n",
    "API_URL = \"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents-corporels-de-la-circulation-millesime/records\"\n",
    "\n",
    "# Nombre d'enregistrements √† r√©cup√©rer par appel (l'API est limit√©e √† 100)\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "print(\"Configuration charg√©e. Pr√™t pour l'extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7564619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration charg√©e. Pr√™t pour l'extraction avanc√©e.\n",
      "D√©but de l'extraction des donn√©es par fen√™tres de 10 000...\n",
      "Fen√™tre de 10000 enregistrements extraite. Total actuel : 10000.\n",
      "Fen√™tre de 10000 enregistrements extraite. Total actuel : 20000.\n",
      "Fen√™tre de 2016 enregistrements extraite. Total actuel : 22016.\n",
      "Derni√®re fen√™tre de donn√©es atteinte. Fin de l'extraction.\n",
      "\n",
      "Extraction termin√©e. 22016 enregistrements bruts ont √©t√© charg√©s.\n",
      "Il reste 22016 enregistrements uniques pr√™ts pour la transformation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_acc</th>\n",
       "      <th>datetime</th>\n",
       "      <th>nom_com</th>\n",
       "      <th>an</th>\n",
       "      <th>mois</th>\n",
       "      <th>jour</th>\n",
       "      <th>hrmn</th>\n",
       "      <th>lum</th>\n",
       "      <th>agg</th>\n",
       "      <th>int</th>\n",
       "      <th>...</th>\n",
       "      <th>dep_name</th>\n",
       "      <th>epci_code</th>\n",
       "      <th>epci_name</th>\n",
       "      <th>reg_code</th>\n",
       "      <th>reg_name</th>\n",
       "      <th>com_arm_name</th>\n",
       "      <th>com_code</th>\n",
       "      <th>coordonnees.lon</th>\n",
       "      <th>coordonnees.lat</th>\n",
       "      <th>coordonnees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201200045901</td>\n",
       "      <td>2012-01-24T19:30:00+00:00</td>\n",
       "      <td>Alfortville</td>\n",
       "      <td>2012</td>\n",
       "      <td>01</td>\n",
       "      <td>24</td>\n",
       "      <td>20:30</td>\n",
       "      <td>Nuit avec √©clairage public allum√©</td>\n",
       "      <td>En agglom√©ration</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Val-de-Marne</td>\n",
       "      <td>249400094</td>\n",
       "      <td>CA Plaine Centrale du Val de Marne</td>\n",
       "      <td>11</td>\n",
       "      <td>√éle-de-France</td>\n",
       "      <td>Alfortville</td>\n",
       "      <td>94002</td>\n",
       "      <td>2.423227</td>\n",
       "      <td>48.798672</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201200038001</td>\n",
       "      <td>2012-04-04T14:05:00+00:00</td>\n",
       "      <td>Pontault-combault</td>\n",
       "      <td>2012</td>\n",
       "      <td>04</td>\n",
       "      <td>04</td>\n",
       "      <td>16:05</td>\n",
       "      <td>Plein jour</td>\n",
       "      <td>En agglom√©ration</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Seine-et-Marne</td>\n",
       "      <td>200022523</td>\n",
       "      <td>CA de la Brie Francilienne</td>\n",
       "      <td>11</td>\n",
       "      <td>√éle-de-France</td>\n",
       "      <td>Pontault-Combault</td>\n",
       "      <td>77373</td>\n",
       "      <td>2.605032</td>\n",
       "      <td>48.797676</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201200000301</td>\n",
       "      <td>2012-01-28T11:00:00+00:00</td>\n",
       "      <td>Pihem</td>\n",
       "      <td>2012</td>\n",
       "      <td>01</td>\n",
       "      <td>28</td>\n",
       "      <td>12:00</td>\n",
       "      <td>Plein jour</td>\n",
       "      <td>Hors agglom√©ration</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Pas-de-Calais</td>\n",
       "      <td>246201016</td>\n",
       "      <td>CC du Pays de Lumbres</td>\n",
       "      <td>31</td>\n",
       "      <td>Nord-Pas-de-Calais</td>\n",
       "      <td>Pihem</td>\n",
       "      <td>62656</td>\n",
       "      <td>2.211844</td>\n",
       "      <td>50.682922</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201200009901</td>\n",
       "      <td>2012-07-15T23:45:00+00:00</td>\n",
       "      <td>Thimert-gatelles</td>\n",
       "      <td>2012</td>\n",
       "      <td>07</td>\n",
       "      <td>16</td>\n",
       "      <td>01:45</td>\n",
       "      <td>Nuit sans √©clairage public</td>\n",
       "      <td>Hors agglom√©ration</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Eure-et-Loir</td>\n",
       "      <td>200040277</td>\n",
       "      <td>CA du Pays de Dreux</td>\n",
       "      <td>24</td>\n",
       "      <td>Centre</td>\n",
       "      <td>Thimert-G√¢telles</td>\n",
       "      <td>28386</td>\n",
       "      <td>1.251383</td>\n",
       "      <td>48.569063</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201200029601</td>\n",
       "      <td>2012-05-30T17:56:00+00:00</td>\n",
       "      <td>Angers</td>\n",
       "      <td>2012</td>\n",
       "      <td>05</td>\n",
       "      <td>30</td>\n",
       "      <td>19:56</td>\n",
       "      <td>Plein jour</td>\n",
       "      <td>En agglom√©ration</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Maine-et-Loire</td>\n",
       "      <td>244900015</td>\n",
       "      <td>CA Angers Loire M√©tropole</td>\n",
       "      <td>52</td>\n",
       "      <td>Pays de la Loire</td>\n",
       "      <td>Angers</td>\n",
       "      <td>49007</td>\n",
       "      <td>-0.540454</td>\n",
       "      <td>47.462673</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        num_acc                   datetime            nom_com    an mois jour  \\\n",
       "0  201200045901  2012-01-24T19:30:00+00:00        Alfortville  2012   01   24   \n",
       "1  201200038001  2012-04-04T14:05:00+00:00  Pontault-combault  2012   04   04   \n",
       "2  201200000301  2012-01-28T11:00:00+00:00              Pihem  2012   01   28   \n",
       "3  201200009901  2012-07-15T23:45:00+00:00   Thimert-gatelles  2012   07   16   \n",
       "4  201200029601  2012-05-30T17:56:00+00:00             Angers  2012   05   30   \n",
       "\n",
       "    hrmn                                lum                 agg int  ...  \\\n",
       "0  20:30  Nuit avec √©clairage public allum√©    En agglom√©ration   1  ...   \n",
       "1  16:05                         Plein jour    En agglom√©ration   1  ...   \n",
       "2  12:00                         Plein jour  Hors agglom√©ration   1  ...   \n",
       "3  01:45         Nuit sans √©clairage public  Hors agglom√©ration   1  ...   \n",
       "4  19:56                         Plein jour    En agglom√©ration   1  ...   \n",
       "\n",
       "         dep_name  epci_code                           epci_name reg_code  \\\n",
       "0    Val-de-Marne  249400094  CA Plaine Centrale du Val de Marne       11   \n",
       "1  Seine-et-Marne  200022523          CA de la Brie Francilienne       11   \n",
       "2   Pas-de-Calais  246201016               CC du Pays de Lumbres       31   \n",
       "3    Eure-et-Loir  200040277                 CA du Pays de Dreux       24   \n",
       "4  Maine-et-Loire  244900015           CA Angers Loire M√©tropole       52   \n",
       "\n",
       "             reg_name       com_arm_name com_code coordonnees.lon  \\\n",
       "0       √éle-de-France        Alfortville    94002        2.423227   \n",
       "1       √éle-de-France  Pontault-Combault    77373        2.605032   \n",
       "2  Nord-Pas-de-Calais              Pihem    62656        2.211844   \n",
       "3              Centre   Thimert-G√¢telles    28386        1.251383   \n",
       "4    Pays de la Loire             Angers    49007       -0.540454   \n",
       "\n",
       "  coordonnees.lat coordonnees  \n",
       "0       48.798672         NaN  \n",
       "1       48.797676         NaN  \n",
       "2       50.682922         NaN  \n",
       "3       48.569063         NaN  \n",
       "4       47.462673         NaN  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "API_URL = \"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents-corporels-de-la-circulation-millesime/records\"\n",
    "BATCH_SIZE = 100\n",
    "# Limite de la fen√™tre de pagination de l'API que nous allons g√©rer\n",
    "PAGINATION_WINDOW_LIMIT = 10000\n",
    "\n",
    "print(\"Configuration charg√©e. Pr√™t pour l'extraction avanc√©e.\")\n",
    "\n",
    "def extract_all_data_with_filter(api_url, batch_size):\n",
    "    \"\"\"\n",
    "    Extrait l'ensemble des donn√©es d'une API OpenDataSoft en contournant\n",
    "    la limite de 10 000 enregistrements par pagination, en utilisant un filtre sur la date.\n",
    "    \"\"\"\n",
    "    all_records = []\n",
    "    last_date = None # Notre \"curseur\" qui servira de marque-page\n",
    "\n",
    "    print(\"D√©but de l'extraction des donn√©es par fen√™tres de 10 000...\")\n",
    "\n",
    "    while True:\n",
    "        # --- Boucle interne pour r√©cup√©rer les enregistrements dans une seule fen√™tre ---\n",
    "        records_in_window = []\n",
    "        offset = 0\n",
    "        \n",
    "        while offset < PAGINATION_WINDOW_LIMIT:\n",
    "            params = {\n",
    "                'limit': batch_size,\n",
    "                'offset': offset,\n",
    "                'order_by': 'date', # Le tri est essentiel pour cette m√©thode\n",
    "            }\n",
    "            # Si nous avons une date de d√©part (apr√®s le premier tour), on l'ajoute comme filtre\n",
    "            if last_date:\n",
    "                # On demande les enregistrements dont la date est sup√©rieure √† notre marque-page\n",
    "                params['where'] = f\"date > '{last_date}'\"\n",
    "\n",
    "            try:\n",
    "                response = requests.get(api_url, params=params)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                batch_records = data.get('results', [])\n",
    "\n",
    "                if not batch_records:\n",
    "                    # Plus de donn√©es dans cette fen√™tre, on arr√™te la boucle interne\n",
    "                    break \n",
    "\n",
    "                records_in_window.extend(batch_records)\n",
    "                offset += batch_size\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Une erreur r√©seau est survenue : {e}\")\n",
    "                return all_records # On retourne ce qu'on a pu r√©cup√©rer\n",
    "\n",
    "        # --- Fin de la boucle interne ---\n",
    "        \n",
    "        if not records_in_window:\n",
    "            # Si la derni√®re fen√™tre √©tait compl√®tement vide, c'est la fin.\n",
    "            print(\"Aucun nouvel enregistrement trouv√©. L'extraction globale est termin√©e.\")\n",
    "            break \n",
    "\n",
    "        all_records.extend(records_in_window)\n",
    "        print(f\"Fen√™tre de {len(records_in_window)} enregistrements extraite. Total actuel : {len(all_records)}.\")\n",
    "\n",
    "        # On met √† jour notre marque-page avec la date du dernier enregistrement r√©cup√©r√©\n",
    "        last_date = records_in_window[-1]['date']\n",
    "\n",
    "        # Condition de sortie finale : si la derni√®re fen√™tre n'√©tait pas pleine,\n",
    "        # cela signifie qu'on a r√©cup√©r√© les derniers enregistrements disponibles.\n",
    "        if len(records_in_window) < PAGINATION_WINDOW_LIMIT:\n",
    "            print(\"Derni√®re fen√™tre de donn√©es atteinte. Fin de l'extraction.\")\n",
    "            break\n",
    "            \n",
    "    return all_records\n",
    "\n",
    "# --- Lancement de l'extraction ---\n",
    "raw_records = extract_all_data_with_filter(API_URL, BATCH_SIZE)\n",
    "\n",
    "# --- Conversion et D√©doublonnage ---\n",
    "if raw_records:\n",
    "    df_raw = pd.json_normalize(raw_records)\n",
    "    print(f\"\\nExtraction termin√©e. {len(df_raw)} enregistrements bruts ont √©t√© charg√©s.\")\n",
    "    \n",
    "    # S√©curit√© : on s'assure qu'il n'y a pas de doublons sur l'identifiant de l'accident\n",
    "    initial_rows = len(df_raw)\n",
    "    df_raw = df_raw.drop_duplicates(subset=['num_acc'])\n",
    "    final_rows = len(df_raw)\n",
    "    \n",
    "    if initial_rows > final_rows:\n",
    "        print(f\"{initial_rows - final_rows} doublons ont √©t√© supprim√©s.\")\n",
    "        \n",
    "    print(f\"Il reste {final_rows} enregistrements uniques pr√™ts pour la transformation.\")\n",
    "else:\n",
    "    print(\"\\nAucun enregistrement n'a √©t√© r√©cup√©r√©.\")\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2fbefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration charg√©e.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from geoalchemy2 import Geometry\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Le \"slug\" du jeu de donn√©es consolid√© et √† jour (2005-2022)\n",
    "DATASET_SLUG = \"bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2022\"\n",
    "API_DATA_GOUV_URL = f\"https://www.data.gouv.fr/api/1/datasets/{DATASET_SLUG}/\"\n",
    "\n",
    "# Dossier o√π les donn√©es brutes seront t√©l√©charg√©es\n",
    "RAW_DATA_FOLDER = 'data_raw'\n",
    "os.makedirs(RAW_DATA_FOLDER, exist_ok=True)\n",
    "\n",
    "print(\"Configuration charg√©e.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e63cd14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_all_accident_files(api_url, download_folder):\n",
    "    \"\"\"\n",
    "    Interroge l'API de data.gouv.fr pour trouver et t√©l√©charger tous les fichiers CSV\n",
    "    des accidents de la route depuis 2005.\n",
    "    \"\"\"\n",
    "    print(\"Interrogation de l'API de data.gouv.fr pour la liste des fichiers...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(api_url)\n",
    "        response.raise_for_status()\n",
    "        dataset_info = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur lors de l'appel √† l'API de data.gouv.fr : {e}\")\n",
    "        return\n",
    "\n",
    "    files_to_download = dataset_info.get('resources', [])\n",
    "    \n",
    "    if not files_to_download:\n",
    "        print(\"Aucun fichier trouv√© dans la r√©ponse de l'API.\")\n",
    "        return\n",
    "\n",
    "    print(f\"{len(files_to_download)} fichiers trouv√©s au total. D√©but du filtrage et du t√©l√©chargement...\")\n",
    "\n",
    "    for file_resource in files_to_download:\n",
    "        # --- BLOC CORRIG√â ---\n",
    "        # On v√©rifie d'abord que le format est bien une cha√Æne de caract√®res\n",
    "        file_format = file_resource.get('format')\n",
    "        is_csv = isinstance(file_format, str) and file_format.lower() == 'csv'\n",
    "        # --------------------\n",
    "\n",
    "        is_schema = 'schema' in file_resource.get('title', '').lower()\n",
    "        \n",
    "        if is_csv and not is_schema:\n",
    "            file_url = file_resource.get('url')\n",
    "            file_title = file_resource.get('title')\n",
    "            \n",
    "            # Correction pour √©viter les doubles extensions .csv.csv\n",
    "            base_name = os.path.basename(file_title).replace(' ', '_')\n",
    "            if base_name.endswith('.csv'):\n",
    "                file_name = base_name\n",
    "            else:\n",
    "                file_name = base_name + '.csv'\n",
    "\n",
    "            file_path = os.path.join(download_folder, file_name)\n",
    "\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"T√©l√©chargement de '{file_title}'...\")\n",
    "                try:\n",
    "                    with requests.get(file_url, stream=True) as r:\n",
    "                        r.raise_for_status()\n",
    "                        with open(file_path, 'wb') as f:\n",
    "                            for chunk in r.iter_content(chunk_size=8192): \n",
    "                                f.write(chunk)\n",
    "                    print(f\" -> Fichier sauvegard√© sous : {file_path}\")\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\" -> ERREUR lors du t√©l√©chargement de {file_url} : {e}\")\n",
    "            else:\n",
    "                print(f\"Fichier '{file_name}' d√©j√† existant. Ignor√©.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b7e1d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- √âtape de Lecture et Transformation ---\n",
      "Fichier 'vehicules-immatricules-baac-2020.csv.csv' charg√©.\n",
      "Fichier 'lieux_2010.csv.csv' charg√©.\n",
      "Fichier 'vehicules_2011.csv.csv' charg√©.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2006.csv.csv : 'utf-8' codec can't decode byte 0xb0 in position 6: invalid start byte\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2016.csv.csv : 'utf-8' codec can't decode byte 0xe8 in position 16: invalid continuation byte\n",
      "Fichier 'usagers_2005.csv.csv' charg√©.\n",
      "Fichier 'usagers_2015.csv.csv' charg√©.\n",
      "Fichier 'lieux_2009.csv.csv' charg√©.\n",
      "Fichier 'usagers-2018.csv.csv' charg√©.\n",
      "Fichier 'vehicules_2008.csv.csv' charg√©.\n",
      "Fichier 'vehicules-2023.csv.csv' charg√©.\n",
      "Fichier 'lieux-2022.csv.csv' charg√©.\n",
      "Fichier 'vehicules-immatricules-baac-2016.csv.csv' charg√©.\n",
      "Fichier 'vehicules_2009.csv.csv' charg√©.\n",
      "Fichier 'usagers-2019.csv.csv' charg√©.\n",
      "Fichier 'lieux_2008.csv.csv' charg√©.\n",
      "Fichier 'lieux-2023.csv.csv' charg√©.\n",
      "Fichier 'vehicules-2022.csv.csv' charg√©.\n",
      "Fichier 'vehicules-immatricules-baac-2017.csv.csv' charg√©.\n",
      "Fichier 'vehicules-immatricules-baac-2021.csv.csv' charg√©.\n",
      "Fichier 'vehicules_2010.csv.csv' charg√©.\n",
      "Fichier 'lieux_2011.csv.csv' charg√©.\n",
      "Fichier 'usagers_2014.csv.csv' charg√©.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2007.csv.csv : 'utf-8' codec can't decode byte 0xe9 in position 1: invalid continuation byte\n",
      "Fichier 'lieux-2021.csv.csv' charg√©.\n",
      "Fichier 'vehicules-2020.csv.csv' charg√©.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques-2018.csv.csv : 'utf-8' codec can't decode byte 0xe9 in position 10: invalid continuation byte\n",
      "Fichier 'vehicules-immatricules-baac-2015.csv.csv' charg√©.\n",
      "Fichier 'vehicules_2012.csv.csv' charg√©.\n",
      "Fichier 'lieux_2013.csv.csv' charg√©.\n",
      "Fichier 'usagers_2006.csv.csv' charg√©.\n",
      "Fichier 'usagers_2016.csv.csv' charg√©.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2005.csv.csv : 'utf-8' codec can't decode byte 0xe9 in position 13: invalid continuation byte\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2015.csv.csv : 'utf-8' codec can't decode byte 0xe8 in position 16: invalid continuation byte\n",
      "Fichier 'lieux-2017.csv.csv' charg√©.\n",
      "Fichier 'lieux_2012.csv.csv' charg√©.\n",
      "Fichier 'vehicules_2013.csv.csv' charg√©.\n",
      "Fichier 'vehicules-immatricules-baac-2022.csv.csv' charg√©.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2014.csv.csv : 'utf-8' codec can't decode byte 0xe9 in position 13: invalid continuation byte\n",
      "Fichier 'usagers_2007.csv.csv' charg√©.\n",
      "Fichier 'vehicules-2017.csv.csv' charg√©.\n",
      "Fichier 'vehicules-2021.csv.csv' charg√©.\n",
      "Fichier 'lieux-2020.csv.csv' charg√©.\n",
      "Fichier 'caracteristiques-2019.csv.csv' charg√©.\n",
      "Fichier 'vehicules-immatricules-baac-2014.csv.csv' charg√©.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2008.csv.csv : 'utf-8' codec can't decode byte 0xf4 in position 17: invalid continuation byte\n",
      "Fichier 'vehicules-immatricules-baac-2011.csv.csv' charg√©.\n",
      "Fichier 'usagers-2020.csv.csv' charg√©.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2011.csv.csv : 'utf-8' codec can't decode byte 0xe9 in position 5: invalid continuation byte\n",
      "Fichier 'usagers_2012.csv.csv' charg√©.\n",
      "Fichier 'vehicules-immatricules-baac-2018.csv.csv' charg√©.\n",
      "Fichier 'lieux_2007.csv.csv' charg√©.\n",
      "Fichier 'vehicules_2006.csv.csv' charg√©.\n",
      "Fichier 'vehicules_2016.csv.csv' charg√©.\n",
      "Fichier 'usagers_2013.csv.csv' charg√©.\n",
      "Fichier 'vehicules-immatricules-baac-2019.csv.csv' charg√©.\n",
      "Fichier 'vehicules-immatricules-baac-2009.csv.csv' charg√©.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2010.csv.csv : 'utf-8' codec can't decode byte 0xe9 in position 8: invalid continuation byte\n",
      "Fichier 'usagers-2017.csv.csv' charg√©.\n",
      "Fichier 'vehicules_2007.csv.csv' charg√©.\n",
      "Fichier 'lieux_2016.csv.csv' charg√©.\n",
      "Fichier 'lieux_2006.csv.csv' charg√©.\n",
      "Fichier 'vehicules-immatricules-baac-2010.csv.csv' charg√©.\n",
      "Fichier 'caracteristiques_2009.csv.csv' charg√©.\n",
      "Fichier 'usagers-2021.csv.csv' charg√©.\n",
      "Fichier 'usagers_2011.csv.csv' charg√©.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2012.csv.csv : 'utf-8' codec can't decode byte 0xe9 in position 8: invalid continuation byte\n",
      "Fichier 'vehicules_2005.csv.csv' charg√©.\n",
      "Fichier 'vehicules_2015.csv.csv' charg√©.\n",
      "Fichier 'lieux_2014.csv.csv' charg√©.\n",
      "Fichier 'caracteristiques-2020.csv.csv' charg√©.\n",
      "Fichier 'usagers-2023.csv.csv' charg√©.\n",
      "Fichier 'vehicules-immatricules-baac-2012.csv.csv' charg√©.\n",
      "Fichier 'usagers_2008.csv.csv' charg√©.\n",
      "Fichier 'vehicules-2018.csv.csv' charg√©.\n",
      "Fichier 'lieux-2019.csv.csv' charg√©.\n",
      "Fichier 'usagers-2022.csv.csv' charg√©.\n",
      "Fichier 'lieux-2018.csv.csv' charg√©.\n",
      "Fichier 'vehicules-immatricules-baac-2013.csv.csv' charg√©.\n",
      "Fichier 'vehicules-2019.csv.csv' charg√©.\n",
      "Fichier 'usagers_2009.csv.csv' charg√©.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2013.csv.csv : 'utf-8' codec can't decode byte 0xe9 in position 16: invalid continuation byte\n",
      "Fichier 'usagers_2010.csv.csv' charg√©.\n",
      "Fichier 'lieux_2015.csv.csv' charg√©.\n",
      "Fichier 'lieux_2005.csv.csv' charg√©.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques-2017.csv.csv : 'utf-8' codec can't decode byte 0xe9 in position 7: invalid continuation byte\n",
      "Fichier 'vehicules_2014.csv.csv' charg√©.\n",
      "\n",
      "Concat√©nation de tous les fichiers par type termin√©e.\n",
      "Total Caract√©ristiques: 180993 lignes\n",
      "Total Lieux: 1247733 lignes\n",
      "Total Usagers: 2762166 lignes\n",
      "Total V√©hicules: 3487680 lignes\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- √âtape de Lecture et Transformation ---\")\n",
    "\n",
    "# Lister tous les fichiers CSV t√©l√©charg√©s\n",
    "all_files = [f for f in os.listdir(RAW_DATA_FOLDER) if f.endswith('.csv')]\n",
    "\n",
    "# Dictionnaires pour stocker les dataframes par type\n",
    "dfs = {'caracteristiques': [], 'lieux': [], 'usagers': [], 'vehicules': []}\n",
    "\n",
    "for file_name in all_files:\n",
    "    file_path = os.path.join(RAW_DATA_FOLDER, file_name)\n",
    "    \n",
    "    # Identifier le type de fichier en se basant sur son nom\n",
    "    if 'caracteristiques' in file_name.lower():\n",
    "        df_type = 'caracteristiques'\n",
    "    elif 'lieux' in file_name.lower():\n",
    "        df_type = 'lieux'\n",
    "    elif 'usagers' in file_name.lower():\n",
    "        df_type = 'usagers'\n",
    "    elif 'vehicules' in file_name.lower():\n",
    "        df_type = 'vehicules'\n",
    "    else:\n",
    "        continue # Ignorer les autres fichiers\n",
    "\n",
    "    # Lire le fichier CSV et l'ajouter √† la liste correspondante\n",
    "    # G√©rer les erreurs de parsing et les diff√©rents s√©parateurs\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=';', encoding='latin1', low_memory=False)\n",
    "        dfs[df_type].append(df)\n",
    "        print(f\"Fichier '{file_name}' charg√©.\")\n",
    "    except Exception as e:\n",
    "        # Essayer avec la virgule comme s√©parateur pour les fichiers plus r√©cents\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=',', encoding='utf-8', low_memory=False)\n",
    "            dfs[df_type].append(df)\n",
    "            print(f\"Fichier '{file_name}' charg√© (avec s√©parateur virgule).\")\n",
    "        except Exception as e2:\n",
    "            print(f\" -> ERREUR de lecture pour le fichier {file_name} : {e2}\")\n",
    "\n",
    "# Concat√©ner tous les dataframes de chaque type\n",
    "df_carac_all = pd.concat(dfs['caracteristiques'], ignore_index=True)\n",
    "df_lieux_all = pd.concat(dfs['lieux'], ignore_index=True)\n",
    "df_usagers_all = pd.concat(dfs['usagers'], ignore_index=True)\n",
    "df_vehicules_all = pd.concat(dfs['vehicules'], ignore_index=True)\n",
    "\n",
    "print(\"\\nConcat√©nation de tous les fichiers par type termin√©e.\")\n",
    "print(f\"Total Caract√©ristiques: {len(df_carac_all)} lignes\")\n",
    "print(f\"Total Lieux: {len(df_lieux_all)} lignes\")\n",
    "print(f\"Total Usagers: {len(df_usagers_all)} lignes\")\n",
    "print(f\"Total V√©hicules: {len(df_vehicules_all)} lignes\")\n",
    "\n",
    "# --- √Ä partir d'ici, on reprend la jointure et la transformation ---\n",
    "# ...\n",
    "# df_merged = pd.merge(df_carac_all, df_lieux_all, ...)\n",
    "# ... etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b72753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELLULE 1 : IMPORTS ET CONFIGURATION\n",
    "# =============================================================================\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from geoalchemy2 import Geometry\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Configuration de l'API ---\n",
    "# Le \"slug\" du jeu de donn√©es consolid√© et √† jour (2005-2022)\n",
    "DATASET_SLUG = \"bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2022\"\n",
    "API_DATA_GOUV_URL = f\"https://www.data.gouv.fr/api/1/datasets/{DATASET_SLUG}/\"\n",
    "\n",
    "# --- Configuration des Dossiers ---\n",
    "RAW_DATA_FOLDER = 'data_raw'\n",
    "os.makedirs(RAW_DATA_FOLDER, exist_ok=True)\n",
    "SCHEMA_FILE_PATH = 'schema/creation_tables.sql'\n",
    "\n",
    "# --- Configuration de la Base de Donn√©es ---\n",
    "db_user = 'user_securite_routiere'\n",
    "db_password = 'password123'\n",
    "db_host = 'localhost'\n",
    "db_port = '5432'\n",
    "db_name = 'securite_routiere_db'\n",
    "connection_string = f\"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\"\n",
    "\n",
    "print(\"‚úÖ Configuration charg√©e. Le script est pr√™t √† √™tre ex√©cut√©.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELLULE 2 : FONCTION DE T√âL√âCHARGEMENT\n",
    "# =============================================================================\n",
    "def download_all_accident_files(api_url, download_folder):\n",
    "    \"\"\"\n",
    "    Interroge l'API de data.gouv.fr pour trouver et t√©l√©charger tous les fichiers CSV\n",
    "    des accidents de la route depuis 2005. Affiche un r√©sum√© √† la fin.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- √âTAPE 1/5 : T√âL√âCHARGEMENT DES DONN√âES BRUTES ---\")\n",
    "    print(\"Interrogation de l'API de data.gouv.fr pour la liste des fichiers...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(api_url)\n",
    "        response.raise_for_status()\n",
    "        dataset_info = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur lors de l'appel √† l'API de data.gouv.fr : {e}\")\n",
    "        return\n",
    "\n",
    "    files_to_download = dataset_info.get('resources', [])\n",
    "    \n",
    "    if not files_to_download:\n",
    "        print(\"Aucun fichier trouv√© dans la r√©ponse de l'API.\")\n",
    "        return\n",
    "\n",
    "    print(f\"{len(files_to_download)} ressources trouv√©es au total. D√©but du filtrage et du t√©l√©chargement...\")\n",
    "    \n",
    "    downloaded_count = 0\n",
    "    ignored_count = 0\n",
    "\n",
    "    for file_resource in files_to_download:\n",
    "        file_format = file_resource.get('format')\n",
    "        is_csv = isinstance(file_format, str) and file_format.lower() == 'csv'\n",
    "        is_schema = 'schema' in file_resource.get('title', '').lower()\n",
    "        \n",
    "        if is_csv and not is_schema:\n",
    "            file_url = file_resource.get('url')\n",
    "            file_title = file_resource.get('title')\n",
    "            \n",
    "            base_name = os.path.basename(file_title).replace(' ', '_')\n",
    "            if base_name.endswith('.csv'):\n",
    "                file_name = base_name\n",
    "            else:\n",
    "                file_name = base_name + '.csv'\n",
    "\n",
    "            file_path = os.path.join(download_folder, file_name)\n",
    "\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"T√©l√©chargement de '{file_title}'...\")\n",
    "                try:\n",
    "                    with requests.get(file_url, stream=True) as r:\n",
    "                        r.raise_for_status()\n",
    "                        with open(file_path, 'wb') as f:\n",
    "                            for chunk in r.iter_content(chunk_size=8192): \n",
    "                                f.write(chunk)\n",
    "                    # print(f\" -> Fichier sauvegard√© sous : {file_path}\")\n",
    "                    downloaded_count += 1\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\" -> ERREUR lors du t√©l√©chargement de {file_url} : {e}\")\n",
    "            else:\n",
    "                ignored_count += 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ   T√âL√âCHARGEMENT DES FICHIERS BRUTS TERMIN√â   ‚úÖ\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"R√©sum√© de l'op√©ration :\")\n",
    "    print(f\" - {downloaded_count} nouveau(x) fichier(s) ont √©t√© t√©l√©charg√©s.\")\n",
    "    print(f\" - {ignored_count} fichier(s) √©taient d√©j√† pr√©sents et ont √©t√© ignor√©s.\")\n",
    "    print(f\" -> Les donn√©es sont pr√™tes pour la prochaine √©tape dans le dossier : '{download_folder}'\")\n",
    "    print(\"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794597fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELLULE 3 : Lancement du T√©l√©chargement\n",
    "# =============================================================================\n",
    "start_time = time.time()\n",
    "download_all_accident_files(API_DATA_GOUV_URL, RAW_DATA_FOLDER)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc51325",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CELLULE 4 : LECTURE ET CONSOLIDATION DES FICHIERS CSV\n",
    "# =============================================================================\n",
    "print(\"\\n--- √âTAPE 2/5 : LECTURE ET CONSOLIDATION DES FICHIERS ---\")\n",
    "all_files = [f for f in os.listdir(RAW_DATA_FOLDER) if f.endswith('.csv')]\n",
    "dfs = {'caracteristiques': [], 'lieux': [], 'usagers': [], 'vehicules': []}\n",
    "\n",
    "for file_name in all_files:\n",
    "    file_path = os.path.join(RAW_DATA_FOLDER, file_name)\n",
    "    df_type = None\n",
    "    if 'caracteristiques' in file_name.lower() or 'caract' in file_name.lower():\n",
    "        df_type = 'caracteristiques'\n",
    "    elif 'lieux' in file_name.lower():\n",
    "        df_type = 'lieux'\n",
    "    elif 'usagers' in file_name.lower():\n",
    "        df_type = 'usagers'\n",
    "    elif 'vehicules' in file_name.lower():\n",
    "        df_type = 'vehicules'\n",
    "    \n",
    "    if df_type:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=';', encoding='latin1', low_memory=False)\n",
    "            dfs[df_type].append(df)\n",
    "        except (UnicodeDecodeError, ValueError):\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep=',', encoding='utf-8', low_memory=False)\n",
    "                dfs[df_type].append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Impossible de lire le fichier {file_name}: {e}\")\n",
    "\n",
    "df_carac = pd.concat(dfs['caracteristiques'], ignore_index=True)\n",
    "df_lieux = pd.concat(dfs['lieux'], ignore_index=True)\n",
    "df_usagers = pd.concat(dfs['usagers'], ignore_index=True)\n",
    "df_vehic = pd.concat(dfs['vehicules'], ignore_index=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ   LECTURE ET CONSOLIDATION TERMIN√âES   ‚úÖ\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Caract√©ristiques: {len(df_carac)} lignes\")\n",
    "print(f\"Total Lieux: {len(df_lieux)} lignes\")\n",
    "print(f\"Total Usagers: {len(df_usagers)} lignes\")\n",
    "print(f\"Total V√©hicules: {len(df_vehic)} lignes\")\n",
    "print(\"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5b661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELLULE 5 : TRANSFORMATION ET MOD√âLISATION\n",
    "# =============================================================================\n",
    "print(\"\\n--- √âTAPE 3/5 : TRANSFORMATION ET MOD√âLISATION ---\")\n",
    "# Jointure\n",
    "print(\"Jointure des dataframes...\")\n",
    "df_merged = pd.merge(df_carac, df_lieux, on='Num_Acc', how='left')\n",
    "df_merged = pd.merge(df_merged, df_vehic, on='Num_Acc', how='left')\n",
    "df_full = pd.merge(df_merged, df_usagers, on=['Num_Acc', 'id_vehicule'], how='left')\n",
    "\n",
    "# Nettoyage et Feature Engineering\n",
    "print(\"Nettoyage et cr√©ation des dimensions...\")\n",
    "# Colonne date\n",
    "df_full['an'] = df_full['an'].apply(lambda x: f\"20{x}\" if len(str(x)) <= 2 else str(x))\n",
    "df_full['mois'] = df_full['mois'].astype(str).str.zfill(2)\n",
    "df_full['jour'] = df_full['jour'].astype(str).str.zfill(2)\n",
    "df_full['hrmn'] = df_full['hrmn'].astype(str).str.zfill(4)\n",
    "df_full['datetime_str'] = df_full['an'] + '-' + df_full['mois'] + '-' + df_full['jour'] + ' ' + df_full['hrmn'].str[:2] + ':' + df_full['hrmn'].str[2:]\n",
    "df_full['datetime'] = pd.to_datetime(df_full['datetime_str'], errors='coerce')\n",
    "\n",
    "# Coordonn√©es (gestion des virgules et conversion)\n",
    "df_full['latitude'] = pd.to_numeric(df_full['lat'].astype(str).str.replace(',', '.'), errors='coerce')\n",
    "df_full['longitude'] = pd.to_numeric(df_full['long'].astype(str).str.replace(',', '.'), errors='coerce')\n",
    "\n",
    "# Remplacer les codes par des libell√©s (exemples)\n",
    "lum_map = {1: 'Plein jour', 2: 'Aube/Cr√©puscule', 3: 'Nuit sans √©clairage', 4: 'Nuit avec √©clairage', 5: 'Nuit avec √©clairage √©teint'}\n",
    "df_full['luminosite'] = df_full['lum'].map(lum_map).fillna('Non renseign√©')\n",
    "\n",
    "# Cr√©ation des DataFrames de dimension\n",
    "D_TEMPS = df_full[['datetime']].copy().dropna().drop_duplicates()\n",
    "D_TEMPS['date'] = D_TEMPS['datetime'].dt.date\n",
    "D_TEMPS['annee'] = D_TEMPS['datetime'].dt.year\n",
    "D_TEMPS['mois'] = D_TEMPS['datetime'].dt.month\n",
    "D_TEMPS['jour_de_la_semaine'] = D_TEMPS['datetime'].dt.day_name(locale='fr_FR.utf8')\n",
    "D_TEMPS['heure'] = D_TEMPS['datetime'].dt.hour\n",
    "D_TEMPS = D_TEMPS.drop(columns=['datetime']).drop_duplicates().reset_index(drop=True).assign(id_temps=lambda x: x.index)\n",
    "\n",
    "# Agr√©gation et cr√©ation de la table de faits\n",
    "print(\"Agr√©gation et cr√©ation de la table de faits...\")\n",
    "grav_map = {1: 'Indemne', 2: 'Tu√©', 3: 'Bless√© hospitalis√©', 4: 'Bless√© l√©ger'}\n",
    "df_full['gravite'] = df_full['grav'].map(grav_map)\n",
    "\n",
    "df_agg = df_full.groupby('Num_Acc').agg(\n",
    "    datetime=('datetime', 'first'),\n",
    "    latitude=('latitude', 'first'),\n",
    "    longitude=('longitude', 'first'),\n",
    "    luminosite=('luminosite', 'first'),\n",
    "    nb_usagers=('id_usager', 'count'),\n",
    "    nb_vehicules=('id_vehicule', lambda x: x.nunique()),\n",
    "    nb_tues=('gravite', lambda x: (x == 'Tu√©').sum()),\n",
    "    nb_blesses_graves=('gravite', lambda x: (x == 'Bless√© hospitalis√©').sum()),\n",
    "    nb_blesses_legers=('gravite', lambda x: (x == 'Bless√© l√©ger').sum())\n",
    ").reset_index()\n",
    "\n",
    "# Jointure pour r√©cup√©rer les cl√©s √©trang√®res\n",
    "df_agg['date'] = df_agg['datetime'].dt.date\n",
    "df_agg['heure'] = df_agg['datetime'].dt.hour\n",
    "df_faits = pd.merge(df_agg, D_TEMPS, on=['date', 'heure'])\n",
    "# (Id√©alement, il faudrait aussi joindre avec D_LIEU et D_CONDITIONS pour les FKs)\n",
    "\n",
    "F_ACCIDENTS = df_faits[['Num_Acc', 'id_temps', 'latitude', 'longitude', 'luminosite', 'nb_usagers', 'nb_vehicules', 'nb_tues', 'nb_blesses_graves', 'nb_blesses_legers']]\n",
    "F_ACCIDENTS = F_ACCIDENTS.rename(columns={'Num_Acc': 'id_accident', 'id_temps': 'id_temps_fk'})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ   TRANSFORMATION ET MOD√âLISATION TERMIN√âES   ‚úÖ\")\n",
    "print(\"=\"*60)\n",
    "print(\"Les DataFrames finaux sont pr√™ts pour le chargement.\")\n",
    "print(\"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b647ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELLULE 6 : CONNEXION ET CR√âATION DU SCH√âMA EN BASE\n",
    "# =============================================================================\n",
    "print(\"\\n--- √âTAPE 4/5 : CONNEXION ET PR√âPARATION DE LA BASE DE DONN√âES ---\")\n",
    "try:\n",
    "    engine = create_engine(connection_string)\n",
    "    with engine.connect() as connection:\n",
    "        print(\"Connexion √† la base de donn√©es PostgreSQL r√©ussie !\")\n",
    "        # Ex√©cuter le script de cr√©ation de tables\n",
    "        with open(SCHEMA_FILE_PATH, 'r') as f:\n",
    "            sql_script = f.read()\n",
    "        connection.execute(text(sql_script))\n",
    "        print(\"Script 'creation_tables.sql' ex√©cut√©. Les tables ont √©t√© (re)cr√©√©es.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de la connexion ou de la cr√©ation du sch√©ma : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af23c1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- √âTAPE 5/5 : CHARGEMENT DES DONN√âES DANS POSTGRESQL ---\n",
      "Chargement de la dimension 'd_temps'...\n",
      "\n",
      "‚ùå Une erreur est survenue lors du chargement des donn√©es : name 'D_TEMPS' is not defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELLULE 7 : CHARGEMENT FINAL EN BASE DE DONN√âES\n",
    "# =============================================================================\n",
    "print(\"\\n--- √âTAPE 5/5 : CHARGEMENT DES DONN√âES DANS POSTGRESQL ---\")\n",
    "try:\n",
    "    # On simplifie en ne chargeant que D_TEMPS et F_ACCIDENTS pour cet exemple\n",
    "    # D'autres dimensions peuvent √™tre cr√©√©es et charg√©es de la m√™me mani√®re\n",
    "    \n",
    "    print(\"Chargement de la dimension 'd_temps'...\")\n",
    "    D_TEMPS.to_sql('d_temps', con=engine, if_exists='replace', index=False)\n",
    "    print(f\" -> OK : {len(D_TEMPS)} lignes charg√©es.\")\n",
    "    \n",
    "    print(\"Chargement de la table de faits 'f_accidents' (peut √™tre long)...\")\n",
    "    # Cr√©ation du WKT (Well-Known Text) pour PostGIS, en g√©rant les NaN\n",
    "    F_ACCIDENTS['geom'] = F_ACCIDENTS.apply(\n",
    "        lambda row: f\"POINT({row['longitude']} {row['latitude']})\" if pd.notna(row['longitude']) and pd.notna(row['latitude']) else None,\n",
    "        axis=1\n",
    "    )\n",
    "    F_ACCIDENTS_TO_LOAD = F_ACCIDENTS.drop(columns=['latitude', 'longitude'])\n",
    "    \n",
    "    F_ACCIDENTS_TO_LOAD.to_sql(\n",
    "        'f_accidents', \n",
    "        con=engine, \n",
    "        if_exists='replace', \n",
    "        index=False,\n",
    "        dtype={'geom': Geometry('POINT', srid=4326)}\n",
    "    )\n",
    "    print(f\" -> OK : {len(F_ACCIDENTS)} lignes charg√©es.\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ   PROCESSUS ETL COMPLET TERMIN√â AVEC SUCC√àS !   üéâ\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Temps d'ex√©cution total : {total_time:.2f} secondes.\")\n",
    "    print(\"Vos donn√©es sont maintenant pr√™tes pour l'analyse dans DBeaver.\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Une erreur est survenue lors du chargement des donn√©es : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b318c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes disponibles :\n",
      "['num_acc', 'datetime', 'nom_com', 'an', 'mois', 'jour', 'hrmn', 'lum', 'agg', 'int', 'atm', 'col', 'dep', 'com', 'insee', 'adr', 'lat', 'long', 'code_postal', 'num', 'pr', 'surf', 'v1', 'circ', 'vosp', 'env1', 'voie', 'larrout', 'v2', 'lartpc', 'nbv', 'catr', 'pr1', 'plan', 'prof', 'infra', 'situ', 'an_nais', 'sexe', 'actp', 'grav', 'secu', 'secu_utl', 'locp', 'num_veh', 'place', 'catu', 'etatp', 'trajet', 'choc', 'manv', 'senc', 'obsm', 'obs', 'catv', 'occutc', 'gps', 'date', 'year_georef', 'com_name', 'dep_code', 'dep_name', 'epci_code', 'epci_name', 'reg_code', 'reg_name', 'com_arm_name', 'com_code', 'coordonnees.lon', 'coordonnees.lat']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_acc</th>\n",
       "      <th>datetime</th>\n",
       "      <th>nom_com</th>\n",
       "      <th>an</th>\n",
       "      <th>mois</th>\n",
       "      <th>jour</th>\n",
       "      <th>hrmn</th>\n",
       "      <th>lum</th>\n",
       "      <th>agg</th>\n",
       "      <th>int</th>\n",
       "      <th>...</th>\n",
       "      <th>dep_code</th>\n",
       "      <th>dep_name</th>\n",
       "      <th>epci_code</th>\n",
       "      <th>epci_name</th>\n",
       "      <th>reg_code</th>\n",
       "      <th>reg_name</th>\n",
       "      <th>com_arm_name</th>\n",
       "      <th>com_code</th>\n",
       "      <th>coordonnees.lon</th>\n",
       "      <th>coordonnees.lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201500012905</td>\n",
       "      <td>2015-01-10T12:15:00+00:00</td>\n",
       "      <td>Fontenay-le-comte</td>\n",
       "      <td>2015</td>\n",
       "      <td>01</td>\n",
       "      <td>10</td>\n",
       "      <td>13:15</td>\n",
       "      <td>Plein jour</td>\n",
       "      <td>Hors agglom√©ration</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>85</td>\n",
       "      <td>Vend√©e</td>\n",
       "      <td>248500092</td>\n",
       "      <td>CC du Pays de Fontenay-Le-Comte</td>\n",
       "      <td>52</td>\n",
       "      <td>Pays de la Loire</td>\n",
       "      <td>Fontenay-le-Comte</td>\n",
       "      <td>85092</td>\n",
       "      <td>-0.806673</td>\n",
       "      <td>46.466325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201500013845</td>\n",
       "      <td>2015-01-22T07:15:00+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>2015</td>\n",
       "      <td>01</td>\n",
       "      <td>22</td>\n",
       "      <td>08:15</td>\n",
       "      <td>Plein jour</td>\n",
       "      <td>En agglom√©ration</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>Bouches-du-Rh√¥ne</td>\n",
       "      <td>241300391</td>\n",
       "      <td>CU de Marseille Provence M√©tropole (Mpm)</td>\n",
       "      <td>93</td>\n",
       "      <td>Provence-Alpes-C√¥te d'Azur</td>\n",
       "      <td>None</td>\n",
       "      <td>13055</td>\n",
       "      <td>5.443675</td>\n",
       "      <td>43.282410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201500013857</td>\n",
       "      <td>2015-04-10T21:30:00+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>2015</td>\n",
       "      <td>04</td>\n",
       "      <td>10</td>\n",
       "      <td>23:30</td>\n",
       "      <td>Nuit avec √©clairage public allum√©</td>\n",
       "      <td>En agglom√©ration</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>Bouches-du-Rh√¥ne</td>\n",
       "      <td>241300391</td>\n",
       "      <td>CU de Marseille Provence M√©tropole (Mpm)</td>\n",
       "      <td>93</td>\n",
       "      <td>Provence-Alpes-C√¥te d'Azur</td>\n",
       "      <td>None</td>\n",
       "      <td>13055</td>\n",
       "      <td>5.374423</td>\n",
       "      <td>43.262794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201500013890</td>\n",
       "      <td>2015-04-19T16:05:00+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>2015</td>\n",
       "      <td>04</td>\n",
       "      <td>19</td>\n",
       "      <td>18:05</td>\n",
       "      <td>Plein jour</td>\n",
       "      <td>En agglom√©ration</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>Bouches-du-Rh√¥ne</td>\n",
       "      <td>241300391</td>\n",
       "      <td>CU de Marseille Provence M√©tropole (Mpm)</td>\n",
       "      <td>93</td>\n",
       "      <td>Provence-Alpes-C√¥te d'Azur</td>\n",
       "      <td>None</td>\n",
       "      <td>13055</td>\n",
       "      <td>5.394886</td>\n",
       "      <td>43.295367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201500017899</td>\n",
       "      <td>2015-05-21T15:00:00+00:00</td>\n",
       "      <td>Hemonstoir</td>\n",
       "      <td>2015</td>\n",
       "      <td>05</td>\n",
       "      <td>21</td>\n",
       "      <td>17:00</td>\n",
       "      <td>Plein jour</td>\n",
       "      <td>Hors agglom√©ration</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>C√¥tes-d'Armor</td>\n",
       "      <td>200042471</td>\n",
       "      <td>CC Cideral</td>\n",
       "      <td>53</td>\n",
       "      <td>Bretagne</td>\n",
       "      <td>H√©monstoir</td>\n",
       "      <td>22075</td>\n",
       "      <td>-2.831244</td>\n",
       "      <td>48.158138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        num_acc                   datetime            nom_com    an mois jour  \\\n",
       "0  201500012905  2015-01-10T12:15:00+00:00  Fontenay-le-comte  2015   01   10   \n",
       "1  201500013845  2015-01-22T07:15:00+00:00               None  2015   01   22   \n",
       "2  201500013857  2015-04-10T21:30:00+00:00               None  2015   04   10   \n",
       "3  201500013890  2015-04-19T16:05:00+00:00               None  2015   04   19   \n",
       "4  201500017899  2015-05-21T15:00:00+00:00         Hemonstoir  2015   05   21   \n",
       "\n",
       "    hrmn                                lum                 agg int  ...  \\\n",
       "0  13:15                         Plein jour  Hors agglom√©ration   1  ...   \n",
       "1  08:15                         Plein jour    En agglom√©ration   1  ...   \n",
       "2  23:30  Nuit avec √©clairage public allum√©    En agglom√©ration   1  ...   \n",
       "3  18:05                         Plein jour    En agglom√©ration   1  ...   \n",
       "4  17:00                         Plein jour  Hors agglom√©ration   2  ...   \n",
       "\n",
       "  dep_code          dep_name  epci_code  \\\n",
       "0       85            Vend√©e  248500092   \n",
       "1       13  Bouches-du-Rh√¥ne  241300391   \n",
       "2       13  Bouches-du-Rh√¥ne  241300391   \n",
       "3       13  Bouches-du-Rh√¥ne  241300391   \n",
       "4       22     C√¥tes-d'Armor  200042471   \n",
       "\n",
       "                                  epci_name reg_code  \\\n",
       "0           CC du Pays de Fontenay-Le-Comte       52   \n",
       "1  CU de Marseille Provence M√©tropole (Mpm)       93   \n",
       "2  CU de Marseille Provence M√©tropole (Mpm)       93   \n",
       "3  CU de Marseille Provence M√©tropole (Mpm)       93   \n",
       "4                                CC Cideral       53   \n",
       "\n",
       "                     reg_name       com_arm_name com_code coordonnees.lon  \\\n",
       "0            Pays de la Loire  Fontenay-le-Comte    85092       -0.806673   \n",
       "1  Provence-Alpes-C√¥te d'Azur               None    13055        5.443675   \n",
       "2  Provence-Alpes-C√¥te d'Azur               None    13055        5.374423   \n",
       "3  Provence-Alpes-C√¥te d'Azur               None    13055        5.394886   \n",
       "4                    Bretagne         H√©monstoir    22075       -2.831244   \n",
       "\n",
       "  coordonnees.lat  \n",
       "0       46.466325  \n",
       "1       43.282410  \n",
       "2       43.262794  \n",
       "3       43.295367  \n",
       "4       48.158138  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# URL de l'API (celle que nous avons identifi√©e pr√©c√©demment)\n",
    "url = \"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents-corporels-de-la-circulation-millesime/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "\n",
    "# Pour la d√©couverte, on ne charge que les 100 premi√®res lignes pour √©viter de tout t√©l√©charger maintenant\n",
    "# Astuce : on utilise 'nrows' de pandas si on lit un fichier local, \n",
    "# mais via API directe c'est plus dur de limiter sans t√©l√©charger.\n",
    "# Pour l'instant, t√©l√©chargeons un petit bout si possible, ou tout si pas le choix.\n",
    "# L'API OpenDataSoft permet parfois de limiter avec 'limit' dans les requ√™tes standard, \n",
    "# mais l'export CSV est souvent total.\n",
    "\n",
    "# Alternative pour la d√©couverte : utiliser l'API de recherche classique pour avoir juste 10 enregistrements JSON\n",
    "url_discovery = \"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents-corporels-de-la-circulation-millesime/records?limit=5\"\n",
    "response = requests.get(url_discovery)\n",
    "data_json = response.json()\n",
    "\n",
    "# Affichons les colonnes disponibles pour analyse\n",
    "if 'results' in data_json:\n",
    "    df_sample = pd.json_normalize(data_json['results'])\n",
    "    print(\"Colonnes disponibles :\")\n",
    "    print(df_sample.columns.tolist())\n",
    "    display(df_sample.head())\n",
    "else:\n",
    "    print(\"Erreur lors de la r√©cup√©ration de l'√©chantillon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae9fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancement du t√©l√©chargement avec l'URL corrig√©e. Veuillez patienter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4_/cb0dgr6d07d_36m06pf2rqjr0000gn/T/ipykernel_1102/114913899.py:20: DtypeWarning: Columns (56) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_complet = pd.read_csv(csv_data, sep=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement et chargement dans le DataFrame termin√©s avec succ√®s !\n",
      "------------------------------------------------------------\n",
      "Le DataFrame final contient 475,911 lignes et 69 colonnes.\n",
      "------------------------------------------------------------\n",
      "Le DataFrame est maintenant stock√© dans la variable 'df'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Identifiant de l'accident</th>\n",
       "      <th>Date et heure</th>\n",
       "      <th>Commune</th>\n",
       "      <th>Ann√©e</th>\n",
       "      <th>Mois</th>\n",
       "      <th>Jour</th>\n",
       "      <th>Heure minute</th>\n",
       "      <th>Lumi√®re</th>\n",
       "      <th>Localisation</th>\n",
       "      <th>Intersection</th>\n",
       "      <th>...</th>\n",
       "      <th>year_georef</th>\n",
       "      <th>Nom Officiel Commune</th>\n",
       "      <th>Code Officiel D√©partement</th>\n",
       "      <th>Nom Officiel D√©partement</th>\n",
       "      <th>Code Officiel EPCI</th>\n",
       "      <th>Nom Officiel EPCI</th>\n",
       "      <th>Code Officiel R√©gion</th>\n",
       "      <th>Nom Officiel R√©gion</th>\n",
       "      <th>Nom Officiel Commune / Arrondissement Municipal</th>\n",
       "      <th>Code Officiel Commune</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201700033619</td>\n",
       "      <td>2017-05-19T17:40:00+02:00</td>\n",
       "      <td>Perpignan</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>17:40</td>\n",
       "      <td>Plein jour</td>\n",
       "      <td>Hors agglom√©ration</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Perpignan</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Pyr√©n√©es-Orientales</td>\n",
       "      <td>200027183.0</td>\n",
       "      <td>CU Perpignan M√©diterran√©e M√©tropole</td>\n",
       "      <td>76.0</td>\n",
       "      <td>Occitanie</td>\n",
       "      <td>Perpignan</td>\n",
       "      <td>66136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201700048262</td>\n",
       "      <td>2017-12-25T17:55:00+01:00</td>\n",
       "      <td>Saint-denis</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>17:55</td>\n",
       "      <td>Cr√©puscule ou aube</td>\n",
       "      <td>Hors agglom√©ration</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Saint-Denis</td>\n",
       "      <td>93.0</td>\n",
       "      <td>Seine-Saint-Denis</td>\n",
       "      <td>200054781.0</td>\n",
       "      <td>M√©tropole du Grand Paris</td>\n",
       "      <td>11.0</td>\n",
       "      <td>√éle-de-France</td>\n",
       "      <td>Saint-Denis</td>\n",
       "      <td>93066.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201700048288</td>\n",
       "      <td>2017-01-01T17:25:00+01:00</td>\n",
       "      <td>Gennevilliers</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17:25</td>\n",
       "      <td>Nuit sans √©clairage public</td>\n",
       "      <td>Hors agglom√©ration</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Gennevilliers</td>\n",
       "      <td>92.0</td>\n",
       "      <td>Hauts-de-Seine</td>\n",
       "      <td>200054781.0</td>\n",
       "      <td>M√©tropole du Grand Paris</td>\n",
       "      <td>11.0</td>\n",
       "      <td>√éle-de-France</td>\n",
       "      <td>Gennevilliers</td>\n",
       "      <td>92036.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201700048486</td>\n",
       "      <td>2017-05-04T00:04:00+02:00</td>\n",
       "      <td>Fontenay-le-fleury</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>00:04</td>\n",
       "      <td>Nuit avec √©clairage public non allum√©</td>\n",
       "      <td>Hors agglom√©ration</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Fontenay-le-Fleury</td>\n",
       "      <td>78.0</td>\n",
       "      <td>Yvelines</td>\n",
       "      <td>247800584.0</td>\n",
       "      <td>CA Versailles Grand Parc (C.A.V.G.P.)</td>\n",
       "      <td>11.0</td>\n",
       "      <td>√éle-de-France</td>\n",
       "      <td>Fontenay-le-Fleury</td>\n",
       "      <td>78242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201700048639</td>\n",
       "      <td>2017-07-11T08:10:00+02:00</td>\n",
       "      <td>Velizy-villacoublay</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>08:10</td>\n",
       "      <td>Plein jour</td>\n",
       "      <td>Hors agglom√©ration</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>V√©lizy-Villacoublay</td>\n",
       "      <td>78.0</td>\n",
       "      <td>Yvelines</td>\n",
       "      <td>247800584.0</td>\n",
       "      <td>CA Versailles Grand Parc (C.A.V.G.P.)</td>\n",
       "      <td>11.0</td>\n",
       "      <td>√éle-de-France</td>\n",
       "      <td>V√©lizy-Villacoublay</td>\n",
       "      <td>78640.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Identifiant de l'accident              Date et heure              Commune  \\\n",
       "0               201700033619  2017-05-19T17:40:00+02:00            Perpignan   \n",
       "1               201700048262  2017-12-25T17:55:00+01:00          Saint-denis   \n",
       "2               201700048288  2017-01-01T17:25:00+01:00        Gennevilliers   \n",
       "3               201700048486  2017-05-04T00:04:00+02:00   Fontenay-le-fleury   \n",
       "4               201700048639  2017-07-11T08:10:00+02:00  Velizy-villacoublay   \n",
       "\n",
       "   Ann√©e  Mois  Jour Heure minute                                Lumi√®re  \\\n",
       "0   2017     5    19        17:40                             Plein jour   \n",
       "1   2017    12    25        17:55                     Cr√©puscule ou aube   \n",
       "2   2017     1     1        17:25             Nuit sans √©clairage public   \n",
       "3   2017     5     4        00:04  Nuit avec √©clairage public non allum√©   \n",
       "4   2017     7    11        08:10                             Plein jour   \n",
       "\n",
       "         Localisation  Intersection  ... year_georef Nom Officiel Commune  \\\n",
       "0  Hors agglom√©ration             6  ...        2017            Perpignan   \n",
       "1  Hors agglom√©ration             1  ...        2017          Saint-Denis   \n",
       "2  Hors agglom√©ration             1  ...        2017        Gennevilliers   \n",
       "3  Hors agglom√©ration             1  ...        2017   Fontenay-le-Fleury   \n",
       "4  Hors agglom√©ration             1  ...        2017  V√©lizy-Villacoublay   \n",
       "\n",
       "  Code Officiel D√©partement Nom Officiel D√©partement  Code Officiel EPCI  \\\n",
       "0                      66.0      Pyr√©n√©es-Orientales         200027183.0   \n",
       "1                      93.0        Seine-Saint-Denis         200054781.0   \n",
       "2                      92.0           Hauts-de-Seine         200054781.0   \n",
       "3                      78.0                 Yvelines         247800584.0   \n",
       "4                      78.0                 Yvelines         247800584.0   \n",
       "\n",
       "                       Nom Officiel EPCI Code Officiel R√©gion  \\\n",
       "0    CU Perpignan M√©diterran√©e M√©tropole                 76.0   \n",
       "1               M√©tropole du Grand Paris                 11.0   \n",
       "2               M√©tropole du Grand Paris                 11.0   \n",
       "3  CA Versailles Grand Parc (C.A.V.G.P.)                 11.0   \n",
       "4  CA Versailles Grand Parc (C.A.V.G.P.)                 11.0   \n",
       "\n",
       "  Nom Officiel R√©gion  Nom Officiel Commune / Arrondissement Municipal  \\\n",
       "0           Occitanie                                        Perpignan   \n",
       "1       √éle-de-France                                      Saint-Denis   \n",
       "2       √éle-de-France                                    Gennevilliers   \n",
       "3       √éle-de-France                               Fontenay-le-Fleury   \n",
       "4       √éle-de-France                              V√©lizy-Villacoublay   \n",
       "\n",
       "   Code Officiel Commune  \n",
       "0                66136.0  \n",
       "1                93066.0  \n",
       "2                92036.0  \n",
       "3                78242.0  \n",
       "4                78640.0  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# URL corrig√©e avec \"corporels\" au lieu de \"corpels\"\n",
    "url_csv = \"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents-corporels-de-la-circulation-millesime/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "\n",
    "print(\"Lancement du t√©l√©chargement avec l'URL corrig√©e. Veuillez patienter...\")\n",
    "\n",
    "try:\n",
    "    # 1. Effectuer la requ√™te pour obtenir les donn√©es\n",
    "    response = requests.get(url_csv)\n",
    "    # L√®ve une exception (comme 404 Not Found ou 500 Server Error) si la requ√™te √©choue\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # 2. Lire le contenu du CSV\n",
    "    csv_data = io.StringIO(response.text)\n",
    "\n",
    "    # 3. Charger les donn√©es dans un DataFrame\n",
    "    df_complet = pd.read_csv(csv_data, sep=';')\n",
    "\n",
    "    print(\"T√©l√©chargement et chargement dans le DataFrame termin√©s avec succ√®s !\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # 4. Afficher le nombre de lignes et de colonnes\n",
    "    nombre_lignes = df_complet.shape[0]\n",
    "    nombre_colonnes = df_complet.shape[1]\n",
    "    \n",
    "    print(f\"Le DataFrame final contient {nombre_lignes:,} lignes et {nombre_colonnes} colonnes.\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Renommer le DataFrame en 'df' pour la suite du projet, c'est plus simple\n",
    "    df = df_complet.copy()\n",
    "    \n",
    "    print(\"Le DataFrame est maintenant stock√© dans la variable 'df'.\")\n",
    "    display(df.head())\n",
    "\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Une erreur de connexion est survenue : {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Une erreur inattendue est survenue : {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
