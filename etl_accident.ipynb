{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46003352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration chargée et moteur de base de données créé.\n"
     ]
    }
   ],
   "source": [
    "# Installer les bibliothèques\n",
    "#pip install jupyterlab pandas requests sqlalchemy psycopg2-binary geopandas\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sqlalchemy import create_engine\n",
    "from shapely.geometry import Point\n",
    "import io\n",
    "\n",
    "# --- CONSTANTES ---\n",
    "# URL de l'API Opendatasoft\n",
    "# Nous limitons à 10000 enregistrements pour cet exemple. Retirez le paramètre &rows=10000 pour tout récupérer.\n",
    "#API_URL = \"https://public.opendatasoft.com/explore/assets/accidents-corporels-de-la-circulation-millesime/\"\n",
    "\n",
    "\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# API\n",
    "API_URL = \"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents-corporels-de-la-circulation-millesime/records\"\n",
    "\n",
    "# Nombre d'enregistrements à récupérer par appel (l'API est limitée à 100)\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Base de données (identifiants de votre docker-compose.yml)\n",
    "DB_USER = \"user_securite_routiere\"\n",
    "DB_PASSWORD = \"password123\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"securite_routiere_db\"\n",
    "DATABASE_URL = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "# Crée le moteur de connexion à la base de données\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "print(\"Configuration chargée et moteur de base de données créé.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24352d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration chargée. Prêt pour l'extraction.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "# URL de l'API pour le jeu de données des accidents corporels\n",
    "API_URL = \"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents-corporels-de-la-circulation-millesime/records\"\n",
    "\n",
    "# Nombre d'enregistrements à récupérer par appel (l'API est limitée à 100)\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "print(\"Configuration chargée. Prêt pour l'extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7564619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration chargée. Prêt pour l'extraction avancée.\n",
      "Début de l'extraction des données par fenêtres de 10 000...\n",
      "Fenêtre de 10000 enregistrements extraite. Total actuel : 10000.\n",
      "Fenêtre de 10000 enregistrements extraite. Total actuel : 20000.\n",
      "Fenêtre de 2016 enregistrements extraite. Total actuel : 22016.\n",
      "Dernière fenêtre de données atteinte. Fin de l'extraction.\n",
      "\n",
      "Extraction terminée. 22016 enregistrements bruts ont été chargés.\n",
      "Il reste 22016 enregistrements uniques prêts pour la transformation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_acc</th>\n",
       "      <th>datetime</th>\n",
       "      <th>nom_com</th>\n",
       "      <th>an</th>\n",
       "      <th>mois</th>\n",
       "      <th>jour</th>\n",
       "      <th>hrmn</th>\n",
       "      <th>lum</th>\n",
       "      <th>agg</th>\n",
       "      <th>int</th>\n",
       "      <th>...</th>\n",
       "      <th>dep_name</th>\n",
       "      <th>epci_code</th>\n",
       "      <th>epci_name</th>\n",
       "      <th>reg_code</th>\n",
       "      <th>reg_name</th>\n",
       "      <th>com_arm_name</th>\n",
       "      <th>com_code</th>\n",
       "      <th>coordonnees.lon</th>\n",
       "      <th>coordonnees.lat</th>\n",
       "      <th>coordonnees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201200045901</td>\n",
       "      <td>2012-01-24T19:30:00+00:00</td>\n",
       "      <td>Alfortville</td>\n",
       "      <td>2012</td>\n",
       "      <td>01</td>\n",
       "      <td>24</td>\n",
       "      <td>20:30</td>\n",
       "      <td>Nuit avec éclairage public allumé</td>\n",
       "      <td>En agglomération</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Val-de-Marne</td>\n",
       "      <td>249400094</td>\n",
       "      <td>CA Plaine Centrale du Val de Marne</td>\n",
       "      <td>11</td>\n",
       "      <td>Île-de-France</td>\n",
       "      <td>Alfortville</td>\n",
       "      <td>94002</td>\n",
       "      <td>2.423227</td>\n",
       "      <td>48.798672</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201200038001</td>\n",
       "      <td>2012-04-04T14:05:00+00:00</td>\n",
       "      <td>Pontault-combault</td>\n",
       "      <td>2012</td>\n",
       "      <td>04</td>\n",
       "      <td>04</td>\n",
       "      <td>16:05</td>\n",
       "      <td>Plein jour</td>\n",
       "      <td>En agglomération</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Seine-et-Marne</td>\n",
       "      <td>200022523</td>\n",
       "      <td>CA de la Brie Francilienne</td>\n",
       "      <td>11</td>\n",
       "      <td>Île-de-France</td>\n",
       "      <td>Pontault-Combault</td>\n",
       "      <td>77373</td>\n",
       "      <td>2.605032</td>\n",
       "      <td>48.797676</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201200000301</td>\n",
       "      <td>2012-01-28T11:00:00+00:00</td>\n",
       "      <td>Pihem</td>\n",
       "      <td>2012</td>\n",
       "      <td>01</td>\n",
       "      <td>28</td>\n",
       "      <td>12:00</td>\n",
       "      <td>Plein jour</td>\n",
       "      <td>Hors agglomération</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Pas-de-Calais</td>\n",
       "      <td>246201016</td>\n",
       "      <td>CC du Pays de Lumbres</td>\n",
       "      <td>31</td>\n",
       "      <td>Nord-Pas-de-Calais</td>\n",
       "      <td>Pihem</td>\n",
       "      <td>62656</td>\n",
       "      <td>2.211844</td>\n",
       "      <td>50.682922</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201200009901</td>\n",
       "      <td>2012-07-15T23:45:00+00:00</td>\n",
       "      <td>Thimert-gatelles</td>\n",
       "      <td>2012</td>\n",
       "      <td>07</td>\n",
       "      <td>16</td>\n",
       "      <td>01:45</td>\n",
       "      <td>Nuit sans éclairage public</td>\n",
       "      <td>Hors agglomération</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Eure-et-Loir</td>\n",
       "      <td>200040277</td>\n",
       "      <td>CA du Pays de Dreux</td>\n",
       "      <td>24</td>\n",
       "      <td>Centre</td>\n",
       "      <td>Thimert-Gâtelles</td>\n",
       "      <td>28386</td>\n",
       "      <td>1.251383</td>\n",
       "      <td>48.569063</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201200029601</td>\n",
       "      <td>2012-05-30T17:56:00+00:00</td>\n",
       "      <td>Angers</td>\n",
       "      <td>2012</td>\n",
       "      <td>05</td>\n",
       "      <td>30</td>\n",
       "      <td>19:56</td>\n",
       "      <td>Plein jour</td>\n",
       "      <td>En agglomération</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Maine-et-Loire</td>\n",
       "      <td>244900015</td>\n",
       "      <td>CA Angers Loire Métropole</td>\n",
       "      <td>52</td>\n",
       "      <td>Pays de la Loire</td>\n",
       "      <td>Angers</td>\n",
       "      <td>49007</td>\n",
       "      <td>-0.540454</td>\n",
       "      <td>47.462673</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        num_acc                   datetime            nom_com    an mois jour  \\\n",
       "0  201200045901  2012-01-24T19:30:00+00:00        Alfortville  2012   01   24   \n",
       "1  201200038001  2012-04-04T14:05:00+00:00  Pontault-combault  2012   04   04   \n",
       "2  201200000301  2012-01-28T11:00:00+00:00              Pihem  2012   01   28   \n",
       "3  201200009901  2012-07-15T23:45:00+00:00   Thimert-gatelles  2012   07   16   \n",
       "4  201200029601  2012-05-30T17:56:00+00:00             Angers  2012   05   30   \n",
       "\n",
       "    hrmn                                lum                 agg int  ...  \\\n",
       "0  20:30  Nuit avec éclairage public allumé    En agglomération   1  ...   \n",
       "1  16:05                         Plein jour    En agglomération   1  ...   \n",
       "2  12:00                         Plein jour  Hors agglomération   1  ...   \n",
       "3  01:45         Nuit sans éclairage public  Hors agglomération   1  ...   \n",
       "4  19:56                         Plein jour    En agglomération   1  ...   \n",
       "\n",
       "         dep_name  epci_code                           epci_name reg_code  \\\n",
       "0    Val-de-Marne  249400094  CA Plaine Centrale du Val de Marne       11   \n",
       "1  Seine-et-Marne  200022523          CA de la Brie Francilienne       11   \n",
       "2   Pas-de-Calais  246201016               CC du Pays de Lumbres       31   \n",
       "3    Eure-et-Loir  200040277                 CA du Pays de Dreux       24   \n",
       "4  Maine-et-Loire  244900015           CA Angers Loire Métropole       52   \n",
       "\n",
       "             reg_name       com_arm_name com_code coordonnees.lon  \\\n",
       "0       Île-de-France        Alfortville    94002        2.423227   \n",
       "1       Île-de-France  Pontault-Combault    77373        2.605032   \n",
       "2  Nord-Pas-de-Calais              Pihem    62656        2.211844   \n",
       "3              Centre   Thimert-Gâtelles    28386        1.251383   \n",
       "4    Pays de la Loire             Angers    49007       -0.540454   \n",
       "\n",
       "  coordonnees.lat coordonnees  \n",
       "0       48.798672         NaN  \n",
       "1       48.797676         NaN  \n",
       "2       50.682922         NaN  \n",
       "3       48.569063         NaN  \n",
       "4       47.462673         NaN  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "API_URL = \"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents-corporels-de-la-circulation-millesime/records\"\n",
    "BATCH_SIZE = 100\n",
    "# Limite de la fenêtre de pagination de l'API que nous allons gérer\n",
    "PAGINATION_WINDOW_LIMIT = 10000\n",
    "\n",
    "print(\"Configuration chargée. Prêt pour l'extraction avancée.\")\n",
    "\n",
    "def extract_all_data_with_filter(api_url, batch_size):\n",
    "    \"\"\"\n",
    "    Extrait l'ensemble des données d'une API OpenDataSoft en contournant\n",
    "    la limite de 10 000 enregistrements par pagination, en utilisant un filtre sur la date.\n",
    "    \"\"\"\n",
    "    all_records = []\n",
    "    last_date = None # Notre \"curseur\" qui servira de marque-page\n",
    "\n",
    "    print(\"Début de l'extraction des données par fenêtres de 10 000...\")\n",
    "\n",
    "    while True:\n",
    "        # --- Boucle interne pour récupérer les enregistrements dans une seule fenêtre ---\n",
    "        records_in_window = []\n",
    "        offset = 0\n",
    "        \n",
    "        while offset < PAGINATION_WINDOW_LIMIT:\n",
    "            params = {\n",
    "                'limit': batch_size,\n",
    "                'offset': offset,\n",
    "                'order_by': 'date', # Le tri est essentiel pour cette méthode\n",
    "            }\n",
    "            # Si nous avons une date de départ (après le premier tour), on l'ajoute comme filtre\n",
    "            if last_date:\n",
    "                # On demande les enregistrements dont la date est supérieure à notre marque-page\n",
    "                params['where'] = f\"date > '{last_date}'\"\n",
    "\n",
    "            try:\n",
    "                response = requests.get(api_url, params=params)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                batch_records = data.get('results', [])\n",
    "\n",
    "                if not batch_records:\n",
    "                    # Plus de données dans cette fenêtre, on arrête la boucle interne\n",
    "                    break \n",
    "\n",
    "                records_in_window.extend(batch_records)\n",
    "                offset += batch_size\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Une erreur réseau est survenue : {e}\")\n",
    "                return all_records # On retourne ce qu'on a pu récupérer\n",
    "\n",
    "        # --- Fin de la boucle interne ---\n",
    "        \n",
    "        if not records_in_window:\n",
    "            # Si la dernière fenêtre était complètement vide, c'est la fin.\n",
    "            print(\"Aucun nouvel enregistrement trouvé. L'extraction globale est terminée.\")\n",
    "            break \n",
    "\n",
    "        all_records.extend(records_in_window)\n",
    "        print(f\"Fenêtre de {len(records_in_window)} enregistrements extraite. Total actuel : {len(all_records)}.\")\n",
    "\n",
    "        # On met à jour notre marque-page avec la date du dernier enregistrement récupéré\n",
    "        last_date = records_in_window[-1]['date']\n",
    "\n",
    "        # Condition de sortie finale : si la dernière fenêtre n'était pas pleine,\n",
    "        # cela signifie qu'on a récupéré les derniers enregistrements disponibles.\n",
    "        if len(records_in_window) < PAGINATION_WINDOW_LIMIT:\n",
    "            print(\"Dernière fenêtre de données atteinte. Fin de l'extraction.\")\n",
    "            break\n",
    "            \n",
    "    return all_records\n",
    "\n",
    "# --- Lancement de l'extraction ---\n",
    "raw_records = extract_all_data_with_filter(API_URL, BATCH_SIZE)\n",
    "\n",
    "# --- Conversion et Dédoublonnage ---\n",
    "if raw_records:\n",
    "    df_raw = pd.json_normalize(raw_records)\n",
    "    print(f\"\\nExtraction terminée. {len(df_raw)} enregistrements bruts ont été chargés.\")\n",
    "    \n",
    "    # Sécurité : on s'assure qu'il n'y a pas de doublons sur l'identifiant de l'accident\n",
    "    initial_rows = len(df_raw)\n",
    "    df_raw = df_raw.drop_duplicates(subset=['num_acc'])\n",
    "    final_rows = len(df_raw)\n",
    "    \n",
    "    if initial_rows > final_rows:\n",
    "        print(f\"{initial_rows - final_rows} doublons ont été supprimés.\")\n",
    "        \n",
    "    print(f\"Il reste {final_rows} enregistrements uniques prêts pour la transformation.\")\n",
    "else:\n",
    "    print(\"\\nAucun enregistrement n'a été récupéré.\")\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2fbefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration chargée.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from geoalchemy2 import Geometry\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Le \"slug\" du jeu de données consolidé et à jour (2005-2022)\n",
    "DATASET_SLUG = \"bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2022\"\n",
    "API_DATA_GOUV_URL = f\"https://www.data.gouv.fr/api/1/datasets/{DATASET_SLUG}/\"\n",
    "\n",
    "# Dossier où les données brutes seront téléchargées\n",
    "RAW_DATA_FOLDER = 'data_raw'\n",
    "os.makedirs(RAW_DATA_FOLDER, exist_ok=True)\n",
    "\n",
    "print(\"Configuration chargée.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e63cd14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_all_accident_files(api_url, download_folder):\n",
    "    \"\"\"\n",
    "    Interroge l'API de data.gouv.fr pour trouver et télécharger tous les fichiers CSV\n",
    "    des accidents de la route depuis 2005.\n",
    "    \"\"\"\n",
    "    print(\"Interrogation de l'API de data.gouv.fr pour la liste des fichiers...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(api_url)\n",
    "        response.raise_for_status()\n",
    "        dataset_info = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur lors de l'appel à l'API de data.gouv.fr : {e}\")\n",
    "        return\n",
    "\n",
    "    files_to_download = dataset_info.get('resources', [])\n",
    "    \n",
    "    if not files_to_download:\n",
    "        print(\"Aucun fichier trouvé dans la réponse de l'API.\")\n",
    "        return\n",
    "\n",
    "    print(f\"{len(files_to_download)} fichiers trouvés au total. Début du filtrage et du téléchargement...\")\n",
    "\n",
    "    for file_resource in files_to_download:\n",
    "        # --- BLOC CORRIGÉ ---\n",
    "        # On vérifie d'abord que le format est bien une chaîne de caractères\n",
    "        file_format = file_resource.get('format')\n",
    "        is_csv = isinstance(file_format, str) and file_format.lower() == 'csv'\n",
    "        # --------------------\n",
    "\n",
    "        is_schema = 'schema' in file_resource.get('title', '').lower()\n",
    "        \n",
    "        if is_csv and not is_schema:\n",
    "            file_url = file_resource.get('url')\n",
    "            file_title = file_resource.get('title')\n",
    "            \n",
    "            # Correction pour éviter les doubles extensions .csv.csv\n",
    "            base_name = os.path.basename(file_title).replace(' ', '_')\n",
    "            if base_name.endswith('.csv'):\n",
    "                file_name = base_name\n",
    "            else:\n",
    "                file_name = base_name + '.csv'\n",
    "\n",
    "            file_path = os.path.join(download_folder, file_name)\n",
    "\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Téléchargement de '{file_title}'...\")\n",
    "                try:\n",
    "                    with requests.get(file_url, stream=True) as r:\n",
    "                        r.raise_for_status()\n",
    "                        with open(file_path, 'wb') as f:\n",
    "                            for chunk in r.iter_content(chunk_size=8192): \n",
    "                                f.write(chunk)\n",
    "                    print(f\" -> Fichier sauvegardé sous : {file_path}\")\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\" -> ERREUR lors du téléchargement de {file_url} : {e}\")\n",
    "            else:\n",
    "                print(f\"Fichier '{file_name}' déjà existant. Ignoré.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b7e1d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Étape de Lecture et Transformation ---\n",
      "Fichier 'vehicules-immatricules-baac-2020.csv.csv' chargé.\n",
      "Fichier 'lieux_2010.csv.csv' chargé.\n",
      "Fichier 'vehicules_2011.csv.csv' chargé.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2006.csv.csv : 'utf-8' codec can't decode byte 0xb0 in position 6: invalid start byte\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2016.csv.csv : 'utf-8' codec can't decode byte 0xe8 in position 16: invalid continuation byte\n",
      "Fichier 'usagers_2005.csv.csv' chargé.\n",
      "Fichier 'usagers_2015.csv.csv' chargé.\n",
      "Fichier 'lieux_2009.csv.csv' chargé.\n",
      "Fichier 'usagers-2018.csv.csv' chargé.\n",
      "Fichier 'vehicules_2008.csv.csv' chargé.\n",
      "Fichier 'vehicules-2023.csv.csv' chargé.\n",
      "Fichier 'lieux-2022.csv.csv' chargé.\n",
      "Fichier 'vehicules-immatricules-baac-2016.csv.csv' chargé.\n",
      "Fichier 'vehicules_2009.csv.csv' chargé.\n",
      "Fichier 'usagers-2019.csv.csv' chargé.\n",
      "Fichier 'lieux_2008.csv.csv' chargé.\n",
      "Fichier 'lieux-2023.csv.csv' chargé.\n",
      "Fichier 'vehicules-2022.csv.csv' chargé.\n",
      "Fichier 'vehicules-immatricules-baac-2017.csv.csv' chargé.\n",
      "Fichier 'vehicules-immatricules-baac-2021.csv.csv' chargé.\n",
      "Fichier 'vehicules_2010.csv.csv' chargé.\n",
      "Fichier 'lieux_2011.csv.csv' chargé.\n",
      "Fichier 'usagers_2014.csv.csv' chargé.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2007.csv.csv : 'utf-8' codec can't decode byte 0xe9 in position 1: invalid continuation byte\n",
      "Fichier 'lieux-2021.csv.csv' chargé.\n",
      "Fichier 'vehicules-2020.csv.csv' chargé.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques-2018.csv.csv : 'utf-8' codec can't decode byte 0xe9 in position 10: invalid continuation byte\n",
      "Fichier 'vehicules-immatricules-baac-2015.csv.csv' chargé.\n",
      "Fichier 'vehicules_2012.csv.csv' chargé.\n",
      "Fichier 'lieux_2013.csv.csv' chargé.\n",
      "Fichier 'usagers_2006.csv.csv' chargé.\n",
      "Fichier 'usagers_2016.csv.csv' chargé.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2005.csv.csv : 'utf-8' codec can't decode byte 0xe9 in position 13: invalid continuation byte\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2015.csv.csv : 'utf-8' codec can't decode byte 0xe8 in position 16: invalid continuation byte\n",
      "Fichier 'lieux-2017.csv.csv' chargé.\n",
      "Fichier 'lieux_2012.csv.csv' chargé.\n",
      "Fichier 'vehicules_2013.csv.csv' chargé.\n",
      "Fichier 'vehicules-immatricules-baac-2022.csv.csv' chargé.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2014.csv.csv : 'utf-8' codec can't decode byte 0xe9 in position 13: invalid continuation byte\n",
      "Fichier 'usagers_2007.csv.csv' chargé.\n",
      "Fichier 'vehicules-2017.csv.csv' chargé.\n",
      "Fichier 'vehicules-2021.csv.csv' chargé.\n",
      "Fichier 'lieux-2020.csv.csv' chargé.\n",
      "Fichier 'caracteristiques-2019.csv.csv' chargé.\n",
      "Fichier 'vehicules-immatricules-baac-2014.csv.csv' chargé.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2008.csv.csv : 'utf-8' codec can't decode byte 0xf4 in position 17: invalid continuation byte\n",
      "Fichier 'vehicules-immatricules-baac-2011.csv.csv' chargé.\n",
      "Fichier 'usagers-2020.csv.csv' chargé.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2011.csv.csv : 'utf-8' codec can't decode byte 0xe9 in position 5: invalid continuation byte\n",
      "Fichier 'usagers_2012.csv.csv' chargé.\n",
      "Fichier 'vehicules-immatricules-baac-2018.csv.csv' chargé.\n",
      "Fichier 'lieux_2007.csv.csv' chargé.\n",
      "Fichier 'vehicules_2006.csv.csv' chargé.\n",
      "Fichier 'vehicules_2016.csv.csv' chargé.\n",
      "Fichier 'usagers_2013.csv.csv' chargé.\n",
      "Fichier 'vehicules-immatricules-baac-2019.csv.csv' chargé.\n",
      "Fichier 'vehicules-immatricules-baac-2009.csv.csv' chargé.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2010.csv.csv : 'utf-8' codec can't decode byte 0xe9 in position 8: invalid continuation byte\n",
      "Fichier 'usagers-2017.csv.csv' chargé.\n",
      "Fichier 'vehicules_2007.csv.csv' chargé.\n",
      "Fichier 'lieux_2016.csv.csv' chargé.\n",
      "Fichier 'lieux_2006.csv.csv' chargé.\n",
      "Fichier 'vehicules-immatricules-baac-2010.csv.csv' chargé.\n",
      "Fichier 'caracteristiques_2009.csv.csv' chargé.\n",
      "Fichier 'usagers-2021.csv.csv' chargé.\n",
      "Fichier 'usagers_2011.csv.csv' chargé.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2012.csv.csv : 'utf-8' codec can't decode byte 0xe9 in position 8: invalid continuation byte\n",
      "Fichier 'vehicules_2005.csv.csv' chargé.\n",
      "Fichier 'vehicules_2015.csv.csv' chargé.\n",
      "Fichier 'lieux_2014.csv.csv' chargé.\n",
      "Fichier 'caracteristiques-2020.csv.csv' chargé.\n",
      "Fichier 'usagers-2023.csv.csv' chargé.\n",
      "Fichier 'vehicules-immatricules-baac-2012.csv.csv' chargé.\n",
      "Fichier 'usagers_2008.csv.csv' chargé.\n",
      "Fichier 'vehicules-2018.csv.csv' chargé.\n",
      "Fichier 'lieux-2019.csv.csv' chargé.\n",
      "Fichier 'usagers-2022.csv.csv' chargé.\n",
      "Fichier 'lieux-2018.csv.csv' chargé.\n",
      "Fichier 'vehicules-immatricules-baac-2013.csv.csv' chargé.\n",
      "Fichier 'vehicules-2019.csv.csv' chargé.\n",
      "Fichier 'usagers_2009.csv.csv' chargé.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques_2013.csv.csv : 'utf-8' codec can't decode byte 0xe9 in position 16: invalid continuation byte\n",
      "Fichier 'usagers_2010.csv.csv' chargé.\n",
      "Fichier 'lieux_2015.csv.csv' chargé.\n",
      "Fichier 'lieux_2005.csv.csv' chargé.\n",
      " -> ERREUR de lecture pour le fichier caracteristiques-2017.csv.csv : 'utf-8' codec can't decode byte 0xe9 in position 7: invalid continuation byte\n",
      "Fichier 'vehicules_2014.csv.csv' chargé.\n",
      "\n",
      "Concaténation de tous les fichiers par type terminée.\n",
      "Total Caractéristiques: 180993 lignes\n",
      "Total Lieux: 1247733 lignes\n",
      "Total Usagers: 2762166 lignes\n",
      "Total Véhicules: 3487680 lignes\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Étape de Lecture et Transformation ---\")\n",
    "\n",
    "# Lister tous les fichiers CSV téléchargés\n",
    "all_files = [f for f in os.listdir(RAW_DATA_FOLDER) if f.endswith('.csv')]\n",
    "\n",
    "# Dictionnaires pour stocker les dataframes par type\n",
    "dfs = {'caracteristiques': [], 'lieux': [], 'usagers': [], 'vehicules': []}\n",
    "\n",
    "for file_name in all_files:\n",
    "    file_path = os.path.join(RAW_DATA_FOLDER, file_name)\n",
    "    \n",
    "    # Identifier le type de fichier en se basant sur son nom\n",
    "    if 'caracteristiques' in file_name.lower():\n",
    "        df_type = 'caracteristiques'\n",
    "    elif 'lieux' in file_name.lower():\n",
    "        df_type = 'lieux'\n",
    "    elif 'usagers' in file_name.lower():\n",
    "        df_type = 'usagers'\n",
    "    elif 'vehicules' in file_name.lower():\n",
    "        df_type = 'vehicules'\n",
    "    else:\n",
    "        continue # Ignorer les autres fichiers\n",
    "\n",
    "    # Lire le fichier CSV et l'ajouter à la liste correspondante\n",
    "    # Gérer les erreurs de parsing et les différents séparateurs\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=';', encoding='latin1', low_memory=False)\n",
    "        dfs[df_type].append(df)\n",
    "        print(f\"Fichier '{file_name}' chargé.\")\n",
    "    except Exception as e:\n",
    "        # Essayer avec la virgule comme séparateur pour les fichiers plus récents\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=',', encoding='utf-8', low_memory=False)\n",
    "            dfs[df_type].append(df)\n",
    "            print(f\"Fichier '{file_name}' chargé (avec séparateur virgule).\")\n",
    "        except Exception as e2:\n",
    "            print(f\" -> ERREUR de lecture pour le fichier {file_name} : {e2}\")\n",
    "\n",
    "# Concaténer tous les dataframes de chaque type\n",
    "df_carac_all = pd.concat(dfs['caracteristiques'], ignore_index=True)\n",
    "df_lieux_all = pd.concat(dfs['lieux'], ignore_index=True)\n",
    "df_usagers_all = pd.concat(dfs['usagers'], ignore_index=True)\n",
    "df_vehicules_all = pd.concat(dfs['vehicules'], ignore_index=True)\n",
    "\n",
    "print(\"\\nConcaténation de tous les fichiers par type terminée.\")\n",
    "print(f\"Total Caractéristiques: {len(df_carac_all)} lignes\")\n",
    "print(f\"Total Lieux: {len(df_lieux_all)} lignes\")\n",
    "print(f\"Total Usagers: {len(df_usagers_all)} lignes\")\n",
    "print(f\"Total Véhicules: {len(df_vehicules_all)} lignes\")\n",
    "\n",
    "# --- À partir d'ici, on reprend la jointure et la transformation ---\n",
    "# ...\n",
    "# df_merged = pd.merge(df_carac_all, df_lieux_all, ...)\n",
    "# ... etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b72753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELLULE 1 : IMPORTS ET CONFIGURATION\n",
    "# =============================================================================\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from geoalchemy2 import Geometry\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Configuration de l'API ---\n",
    "# Le \"slug\" du jeu de données consolidé et à jour (2005-2022)\n",
    "DATASET_SLUG = \"bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2022\"\n",
    "API_DATA_GOUV_URL = f\"https://www.data.gouv.fr/api/1/datasets/{DATASET_SLUG}/\"\n",
    "\n",
    "# --- Configuration des Dossiers ---\n",
    "RAW_DATA_FOLDER = 'data_raw'\n",
    "os.makedirs(RAW_DATA_FOLDER, exist_ok=True)\n",
    "SCHEMA_FILE_PATH = 'schema/creation_tables.sql'\n",
    "\n",
    "# --- Configuration de la Base de Données ---\n",
    "db_user = 'user_securite_routiere'\n",
    "db_password = 'password123'\n",
    "db_host = 'localhost'\n",
    "db_port = '5432'\n",
    "db_name = 'securite_routiere_db'\n",
    "connection_string = f\"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\"\n",
    "\n",
    "print(\"✅ Configuration chargée. Le script est prêt à être exécuté.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELLULE 2 : FONCTION DE TÉLÉCHARGEMENT\n",
    "# =============================================================================\n",
    "def download_all_accident_files(api_url, download_folder):\n",
    "    \"\"\"\n",
    "    Interroge l'API de data.gouv.fr pour trouver et télécharger tous les fichiers CSV\n",
    "    des accidents de la route depuis 2005. Affiche un résumé à la fin.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- ÉTAPE 1/5 : TÉLÉCHARGEMENT DES DONNÉES BRUTES ---\")\n",
    "    print(\"Interrogation de l'API de data.gouv.fr pour la liste des fichiers...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(api_url)\n",
    "        response.raise_for_status()\n",
    "        dataset_info = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur lors de l'appel à l'API de data.gouv.fr : {e}\")\n",
    "        return\n",
    "\n",
    "    files_to_download = dataset_info.get('resources', [])\n",
    "    \n",
    "    if not files_to_download:\n",
    "        print(\"Aucun fichier trouvé dans la réponse de l'API.\")\n",
    "        return\n",
    "\n",
    "    print(f\"{len(files_to_download)} ressources trouvées au total. Début du filtrage et du téléchargement...\")\n",
    "    \n",
    "    downloaded_count = 0\n",
    "    ignored_count = 0\n",
    "\n",
    "    for file_resource in files_to_download:\n",
    "        file_format = file_resource.get('format')\n",
    "        is_csv = isinstance(file_format, str) and file_format.lower() == 'csv'\n",
    "        is_schema = 'schema' in file_resource.get('title', '').lower()\n",
    "        \n",
    "        if is_csv and not is_schema:\n",
    "            file_url = file_resource.get('url')\n",
    "            file_title = file_resource.get('title')\n",
    "            \n",
    "            base_name = os.path.basename(file_title).replace(' ', '_')\n",
    "            if base_name.endswith('.csv'):\n",
    "                file_name = base_name\n",
    "            else:\n",
    "                file_name = base_name + '.csv'\n",
    "\n",
    "            file_path = os.path.join(download_folder, file_name)\n",
    "\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Téléchargement de '{file_title}'...\")\n",
    "                try:\n",
    "                    with requests.get(file_url, stream=True) as r:\n",
    "                        r.raise_for_status()\n",
    "                        with open(file_path, 'wb') as f:\n",
    "                            for chunk in r.iter_content(chunk_size=8192): \n",
    "                                f.write(chunk)\n",
    "                    # print(f\" -> Fichier sauvegardé sous : {file_path}\")\n",
    "                    downloaded_count += 1\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\" -> ERREUR lors du téléchargement de {file_url} : {e}\")\n",
    "            else:\n",
    "                ignored_count += 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✅   TÉLÉCHARGEMENT DES FICHIERS BRUTS TERMINÉ   ✅\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Résumé de l'opération :\")\n",
    "    print(f\" - {downloaded_count} nouveau(x) fichier(s) ont été téléchargés.\")\n",
    "    print(f\" - {ignored_count} fichier(s) étaient déjà présents et ont été ignorés.\")\n",
    "    print(f\" -> Les données sont prêtes pour la prochaine étape dans le dossier : '{download_folder}'\")\n",
    "    print(\"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794597fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELLULE 3 : Lancement du Téléchargement\n",
    "# =============================================================================\n",
    "start_time = time.time()\n",
    "download_all_accident_files(API_DATA_GOUV_URL, RAW_DATA_FOLDER)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc51325",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CELLULE 4 : LECTURE ET CONSOLIDATION DES FICHIERS CSV\n",
    "# =============================================================================\n",
    "print(\"\\n--- ÉTAPE 2/5 : LECTURE ET CONSOLIDATION DES FICHIERS ---\")\n",
    "all_files = [f for f in os.listdir(RAW_DATA_FOLDER) if f.endswith('.csv')]\n",
    "dfs = {'caracteristiques': [], 'lieux': [], 'usagers': [], 'vehicules': []}\n",
    "\n",
    "for file_name in all_files:\n",
    "    file_path = os.path.join(RAW_DATA_FOLDER, file_name)\n",
    "    df_type = None\n",
    "    if 'caracteristiques' in file_name.lower() or 'caract' in file_name.lower():\n",
    "        df_type = 'caracteristiques'\n",
    "    elif 'lieux' in file_name.lower():\n",
    "        df_type = 'lieux'\n",
    "    elif 'usagers' in file_name.lower():\n",
    "        df_type = 'usagers'\n",
    "    elif 'vehicules' in file_name.lower():\n",
    "        df_type = 'vehicules'\n",
    "    \n",
    "    if df_type:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=';', encoding='latin1', low_memory=False)\n",
    "            dfs[df_type].append(df)\n",
    "        except (UnicodeDecodeError, ValueError):\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep=',', encoding='utf-8', low_memory=False)\n",
    "                dfs[df_type].append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Impossible de lire le fichier {file_name}: {e}\")\n",
    "\n",
    "df_carac = pd.concat(dfs['caracteristiques'], ignore_index=True)\n",
    "df_lieux = pd.concat(dfs['lieux'], ignore_index=True)\n",
    "df_usagers = pd.concat(dfs['usagers'], ignore_index=True)\n",
    "df_vehic = pd.concat(dfs['vehicules'], ignore_index=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅   LECTURE ET CONSOLIDATION TERMINÉES   ✅\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Caractéristiques: {len(df_carac)} lignes\")\n",
    "print(f\"Total Lieux: {len(df_lieux)} lignes\")\n",
    "print(f\"Total Usagers: {len(df_usagers)} lignes\")\n",
    "print(f\"Total Véhicules: {len(df_vehic)} lignes\")\n",
    "print(\"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5b661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELLULE 5 : TRANSFORMATION ET MODÉLISATION\n",
    "# =============================================================================\n",
    "print(\"\\n--- ÉTAPE 3/5 : TRANSFORMATION ET MODÉLISATION ---\")\n",
    "# Jointure\n",
    "print(\"Jointure des dataframes...\")\n",
    "df_merged = pd.merge(df_carac, df_lieux, on='Num_Acc', how='left')\n",
    "df_merged = pd.merge(df_merged, df_vehic, on='Num_Acc', how='left')\n",
    "df_full = pd.merge(df_merged, df_usagers, on=['Num_Acc', 'id_vehicule'], how='left')\n",
    "\n",
    "# Nettoyage et Feature Engineering\n",
    "print(\"Nettoyage et création des dimensions...\")\n",
    "# Colonne date\n",
    "df_full['an'] = df_full['an'].apply(lambda x: f\"20{x}\" if len(str(x)) <= 2 else str(x))\n",
    "df_full['mois'] = df_full['mois'].astype(str).str.zfill(2)\n",
    "df_full['jour'] = df_full['jour'].astype(str).str.zfill(2)\n",
    "df_full['hrmn'] = df_full['hrmn'].astype(str).str.zfill(4)\n",
    "df_full['datetime_str'] = df_full['an'] + '-' + df_full['mois'] + '-' + df_full['jour'] + ' ' + df_full['hrmn'].str[:2] + ':' + df_full['hrmn'].str[2:]\n",
    "df_full['datetime'] = pd.to_datetime(df_full['datetime_str'], errors='coerce')\n",
    "\n",
    "# Coordonnées (gestion des virgules et conversion)\n",
    "df_full['latitude'] = pd.to_numeric(df_full['lat'].astype(str).str.replace(',', '.'), errors='coerce')\n",
    "df_full['longitude'] = pd.to_numeric(df_full['long'].astype(str).str.replace(',', '.'), errors='coerce')\n",
    "\n",
    "# Remplacer les codes par des libellés (exemples)\n",
    "lum_map = {1: 'Plein jour', 2: 'Aube/Crépuscule', 3: 'Nuit sans éclairage', 4: 'Nuit avec éclairage', 5: 'Nuit avec éclairage éteint'}\n",
    "df_full['luminosite'] = df_full['lum'].map(lum_map).fillna('Non renseigné')\n",
    "\n",
    "# Création des DataFrames de dimension\n",
    "D_TEMPS = df_full[['datetime']].copy().dropna().drop_duplicates()\n",
    "D_TEMPS['date'] = D_TEMPS['datetime'].dt.date\n",
    "D_TEMPS['annee'] = D_TEMPS['datetime'].dt.year\n",
    "D_TEMPS['mois'] = D_TEMPS['datetime'].dt.month\n",
    "D_TEMPS['jour_de_la_semaine'] = D_TEMPS['datetime'].dt.day_name(locale='fr_FR.utf8')\n",
    "D_TEMPS['heure'] = D_TEMPS['datetime'].dt.hour\n",
    "D_TEMPS = D_TEMPS.drop(columns=['datetime']).drop_duplicates().reset_index(drop=True).assign(id_temps=lambda x: x.index)\n",
    "\n",
    "# Agrégation et création de la table de faits\n",
    "print(\"Agrégation et création de la table de faits...\")\n",
    "grav_map = {1: 'Indemne', 2: 'Tué', 3: 'Blessé hospitalisé', 4: 'Blessé léger'}\n",
    "df_full['gravite'] = df_full['grav'].map(grav_map)\n",
    "\n",
    "df_agg = df_full.groupby('Num_Acc').agg(\n",
    "    datetime=('datetime', 'first'),\n",
    "    latitude=('latitude', 'first'),\n",
    "    longitude=('longitude', 'first'),\n",
    "    luminosite=('luminosite', 'first'),\n",
    "    nb_usagers=('id_usager', 'count'),\n",
    "    nb_vehicules=('id_vehicule', lambda x: x.nunique()),\n",
    "    nb_tues=('gravite', lambda x: (x == 'Tué').sum()),\n",
    "    nb_blesses_graves=('gravite', lambda x: (x == 'Blessé hospitalisé').sum()),\n",
    "    nb_blesses_legers=('gravite', lambda x: (x == 'Blessé léger').sum())\n",
    ").reset_index()\n",
    "\n",
    "# Jointure pour récupérer les clés étrangères\n",
    "df_agg['date'] = df_agg['datetime'].dt.date\n",
    "df_agg['heure'] = df_agg['datetime'].dt.hour\n",
    "df_faits = pd.merge(df_agg, D_TEMPS, on=['date', 'heure'])\n",
    "# (Idéalement, il faudrait aussi joindre avec D_LIEU et D_CONDITIONS pour les FKs)\n",
    "\n",
    "F_ACCIDENTS = df_faits[['Num_Acc', 'id_temps', 'latitude', 'longitude', 'luminosite', 'nb_usagers', 'nb_vehicules', 'nb_tues', 'nb_blesses_graves', 'nb_blesses_legers']]\n",
    "F_ACCIDENTS = F_ACCIDENTS.rename(columns={'Num_Acc': 'id_accident', 'id_temps': 'id_temps_fk'})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅   TRANSFORMATION ET MODÉLISATION TERMINÉES   ✅\")\n",
    "print(\"=\"*60)\n",
    "print(\"Les DataFrames finaux sont prêts pour le chargement.\")\n",
    "print(\"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b647ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELLULE 6 : CONNEXION ET CRÉATION DU SCHÉMA EN BASE\n",
    "# =============================================================================\n",
    "print(\"\\n--- ÉTAPE 4/5 : CONNEXION ET PRÉPARATION DE LA BASE DE DONNÉES ---\")\n",
    "try:\n",
    "    engine = create_engine(connection_string)\n",
    "    with engine.connect() as connection:\n",
    "        print(\"Connexion à la base de données PostgreSQL réussie !\")\n",
    "        # Exécuter le script de création de tables\n",
    "        with open(SCHEMA_FILE_PATH, 'r') as f:\n",
    "            sql_script = f.read()\n",
    "        connection.execute(text(sql_script))\n",
    "        print(\"Script 'creation_tables.sql' exécuté. Les tables ont été (re)créées.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur lors de la connexion ou de la création du schéma : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af23c1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ÉTAPE 5/5 : CHARGEMENT DES DONNÉES DANS POSTGRESQL ---\n",
      "Chargement de la dimension 'd_temps'...\n",
      "\n",
      "❌ Une erreur est survenue lors du chargement des données : name 'D_TEMPS' is not defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELLULE 7 : CHARGEMENT FINAL EN BASE DE DONNÉES\n",
    "# =============================================================================\n",
    "print(\"\\n--- ÉTAPE 5/5 : CHARGEMENT DES DONNÉES DANS POSTGRESQL ---\")\n",
    "try:\n",
    "    # On simplifie en ne chargeant que D_TEMPS et F_ACCIDENTS pour cet exemple\n",
    "    # D'autres dimensions peuvent être créées et chargées de la même manière\n",
    "    \n",
    "    print(\"Chargement de la dimension 'd_temps'...\")\n",
    "    D_TEMPS.to_sql('d_temps', con=engine, if_exists='replace', index=False)\n",
    "    print(f\" -> OK : {len(D_TEMPS)} lignes chargées.\")\n",
    "    \n",
    "    print(\"Chargement de la table de faits 'f_accidents' (peut être long)...\")\n",
    "    # Création du WKT (Well-Known Text) pour PostGIS, en gérant les NaN\n",
    "    F_ACCIDENTS['geom'] = F_ACCIDENTS.apply(\n",
    "        lambda row: f\"POINT({row['longitude']} {row['latitude']})\" if pd.notna(row['longitude']) and pd.notna(row['latitude']) else None,\n",
    "        axis=1\n",
    "    )\n",
    "    F_ACCIDENTS_TO_LOAD = F_ACCIDENTS.drop(columns=['latitude', 'longitude'])\n",
    "    \n",
    "    F_ACCIDENTS_TO_LOAD.to_sql(\n",
    "        'f_accidents', \n",
    "        con=engine, \n",
    "        if_exists='replace', \n",
    "        index=False,\n",
    "        dtype={'geom': Geometry('POINT', srid=4326)}\n",
    "    )\n",
    "    print(f\" -> OK : {len(F_ACCIDENTS)} lignes chargées.\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎉   PROCESSUS ETL COMPLET TERMINÉ AVEC SUCCÈS !   🎉\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Temps d'exécution total : {total_time:.2f} secondes.\")\n",
    "    print(\"Vos données sont maintenant prêtes pour l'analyse dans DBeaver.\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Une erreur est survenue lors du chargement des données : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b318c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes disponibles :\n",
      "['num_acc', 'datetime', 'nom_com', 'an', 'mois', 'jour', 'hrmn', 'lum', 'agg', 'int', 'atm', 'col', 'dep', 'com', 'insee', 'adr', 'lat', 'long', 'code_postal', 'num', 'pr', 'surf', 'v1', 'circ', 'vosp', 'env1', 'voie', 'larrout', 'v2', 'lartpc', 'nbv', 'catr', 'pr1', 'plan', 'prof', 'infra', 'situ', 'an_nais', 'sexe', 'actp', 'grav', 'secu', 'secu_utl', 'locp', 'num_veh', 'place', 'catu', 'etatp', 'trajet', 'choc', 'manv', 'senc', 'obsm', 'obs', 'catv', 'occutc', 'gps', 'date', 'year_georef', 'com_name', 'dep_code', 'dep_name', 'epci_code', 'epci_name', 'reg_code', 'reg_name', 'com_arm_name', 'com_code', 'coordonnees.lon', 'coordonnees.lat']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_acc</th>\n",
       "      <th>datetime</th>\n",
       "      <th>nom_com</th>\n",
       "      <th>an</th>\n",
       "      <th>mois</th>\n",
       "      <th>jour</th>\n",
       "      <th>hrmn</th>\n",
       "      <th>lum</th>\n",
       "      <th>agg</th>\n",
       "      <th>int</th>\n",
       "      <th>...</th>\n",
       "      <th>dep_code</th>\n",
       "      <th>dep_name</th>\n",
       "      <th>epci_code</th>\n",
       "      <th>epci_name</th>\n",
       "      <th>reg_code</th>\n",
       "      <th>reg_name</th>\n",
       "      <th>com_arm_name</th>\n",
       "      <th>com_code</th>\n",
       "      <th>coordonnees.lon</th>\n",
       "      <th>coordonnees.lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201500012905</td>\n",
       "      <td>2015-01-10T12:15:00+00:00</td>\n",
       "      <td>Fontenay-le-comte</td>\n",
       "      <td>2015</td>\n",
       "      <td>01</td>\n",
       "      <td>10</td>\n",
       "      <td>13:15</td>\n",
       "      <td>Plein jour</td>\n",
       "      <td>Hors agglomération</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>85</td>\n",
       "      <td>Vendée</td>\n",
       "      <td>248500092</td>\n",
       "      <td>CC du Pays de Fontenay-Le-Comte</td>\n",
       "      <td>52</td>\n",
       "      <td>Pays de la Loire</td>\n",
       "      <td>Fontenay-le-Comte</td>\n",
       "      <td>85092</td>\n",
       "      <td>-0.806673</td>\n",
       "      <td>46.466325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201500013845</td>\n",
       "      <td>2015-01-22T07:15:00+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>2015</td>\n",
       "      <td>01</td>\n",
       "      <td>22</td>\n",
       "      <td>08:15</td>\n",
       "      <td>Plein jour</td>\n",
       "      <td>En agglomération</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>Bouches-du-Rhône</td>\n",
       "      <td>241300391</td>\n",
       "      <td>CU de Marseille Provence Métropole (Mpm)</td>\n",
       "      <td>93</td>\n",
       "      <td>Provence-Alpes-Côte d'Azur</td>\n",
       "      <td>None</td>\n",
       "      <td>13055</td>\n",
       "      <td>5.443675</td>\n",
       "      <td>43.282410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201500013857</td>\n",
       "      <td>2015-04-10T21:30:00+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>2015</td>\n",
       "      <td>04</td>\n",
       "      <td>10</td>\n",
       "      <td>23:30</td>\n",
       "      <td>Nuit avec éclairage public allumé</td>\n",
       "      <td>En agglomération</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>Bouches-du-Rhône</td>\n",
       "      <td>241300391</td>\n",
       "      <td>CU de Marseille Provence Métropole (Mpm)</td>\n",
       "      <td>93</td>\n",
       "      <td>Provence-Alpes-Côte d'Azur</td>\n",
       "      <td>None</td>\n",
       "      <td>13055</td>\n",
       "      <td>5.374423</td>\n",
       "      <td>43.262794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201500013890</td>\n",
       "      <td>2015-04-19T16:05:00+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>2015</td>\n",
       "      <td>04</td>\n",
       "      <td>19</td>\n",
       "      <td>18:05</td>\n",
       "      <td>Plein jour</td>\n",
       "      <td>En agglomération</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>Bouches-du-Rhône</td>\n",
       "      <td>241300391</td>\n",
       "      <td>CU de Marseille Provence Métropole (Mpm)</td>\n",
       "      <td>93</td>\n",
       "      <td>Provence-Alpes-Côte d'Azur</td>\n",
       "      <td>None</td>\n",
       "      <td>13055</td>\n",
       "      <td>5.394886</td>\n",
       "      <td>43.295367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201500017899</td>\n",
       "      <td>2015-05-21T15:00:00+00:00</td>\n",
       "      <td>Hemonstoir</td>\n",
       "      <td>2015</td>\n",
       "      <td>05</td>\n",
       "      <td>21</td>\n",
       "      <td>17:00</td>\n",
       "      <td>Plein jour</td>\n",
       "      <td>Hors agglomération</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>Côtes-d'Armor</td>\n",
       "      <td>200042471</td>\n",
       "      <td>CC Cideral</td>\n",
       "      <td>53</td>\n",
       "      <td>Bretagne</td>\n",
       "      <td>Hémonstoir</td>\n",
       "      <td>22075</td>\n",
       "      <td>-2.831244</td>\n",
       "      <td>48.158138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        num_acc                   datetime            nom_com    an mois jour  \\\n",
       "0  201500012905  2015-01-10T12:15:00+00:00  Fontenay-le-comte  2015   01   10   \n",
       "1  201500013845  2015-01-22T07:15:00+00:00               None  2015   01   22   \n",
       "2  201500013857  2015-04-10T21:30:00+00:00               None  2015   04   10   \n",
       "3  201500013890  2015-04-19T16:05:00+00:00               None  2015   04   19   \n",
       "4  201500017899  2015-05-21T15:00:00+00:00         Hemonstoir  2015   05   21   \n",
       "\n",
       "    hrmn                                lum                 agg int  ...  \\\n",
       "0  13:15                         Plein jour  Hors agglomération   1  ...   \n",
       "1  08:15                         Plein jour    En agglomération   1  ...   \n",
       "2  23:30  Nuit avec éclairage public allumé    En agglomération   1  ...   \n",
       "3  18:05                         Plein jour    En agglomération   1  ...   \n",
       "4  17:00                         Plein jour  Hors agglomération   2  ...   \n",
       "\n",
       "  dep_code          dep_name  epci_code  \\\n",
       "0       85            Vendée  248500092   \n",
       "1       13  Bouches-du-Rhône  241300391   \n",
       "2       13  Bouches-du-Rhône  241300391   \n",
       "3       13  Bouches-du-Rhône  241300391   \n",
       "4       22     Côtes-d'Armor  200042471   \n",
       "\n",
       "                                  epci_name reg_code  \\\n",
       "0           CC du Pays de Fontenay-Le-Comte       52   \n",
       "1  CU de Marseille Provence Métropole (Mpm)       93   \n",
       "2  CU de Marseille Provence Métropole (Mpm)       93   \n",
       "3  CU de Marseille Provence Métropole (Mpm)       93   \n",
       "4                                CC Cideral       53   \n",
       "\n",
       "                     reg_name       com_arm_name com_code coordonnees.lon  \\\n",
       "0            Pays de la Loire  Fontenay-le-Comte    85092       -0.806673   \n",
       "1  Provence-Alpes-Côte d'Azur               None    13055        5.443675   \n",
       "2  Provence-Alpes-Côte d'Azur               None    13055        5.374423   \n",
       "3  Provence-Alpes-Côte d'Azur               None    13055        5.394886   \n",
       "4                    Bretagne         Hémonstoir    22075       -2.831244   \n",
       "\n",
       "  coordonnees.lat  \n",
       "0       46.466325  \n",
       "1       43.282410  \n",
       "2       43.262794  \n",
       "3       43.295367  \n",
       "4       48.158138  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# URL de l'API (celle que nous avons identifiée précédemment)\n",
    "url = \"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents-corporels-de-la-circulation-millesime/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "\n",
    "# Pour la découverte, on ne charge que les 100 premières lignes pour éviter de tout télécharger maintenant\n",
    "# Astuce : on utilise 'nrows' de pandas si on lit un fichier local, \n",
    "# mais via API directe c'est plus dur de limiter sans télécharger.\n",
    "# Pour l'instant, téléchargeons un petit bout si possible, ou tout si pas le choix.\n",
    "# L'API OpenDataSoft permet parfois de limiter avec 'limit' dans les requêtes standard, \n",
    "# mais l'export CSV est souvent total.\n",
    "\n",
    "# Alternative pour la découverte : utiliser l'API de recherche classique pour avoir juste 10 enregistrements JSON\n",
    "url_discovery = \"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents-corporels-de-la-circulation-millesime/records?limit=5\"\n",
    "response = requests.get(url_discovery)\n",
    "data_json = response.json()\n",
    "\n",
    "# Affichons les colonnes disponibles pour analyse\n",
    "if 'results' in data_json:\n",
    "    df_sample = pd.json_normalize(data_json['results'])\n",
    "    print(\"Colonnes disponibles :\")\n",
    "    print(df_sample.columns.tolist())\n",
    "    display(df_sample.head())\n",
    "else:\n",
    "    print(\"Erreur lors de la récupération de l'échantillon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae9fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancement du téléchargement avec l'URL corrigée. Veuillez patienter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4_/cb0dgr6d07d_36m06pf2rqjr0000gn/T/ipykernel_1102/114913899.py:20: DtypeWarning: Columns (56) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_complet = pd.read_csv(csv_data, sep=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement et chargement dans le DataFrame terminés avec succès !\n",
      "------------------------------------------------------------\n",
      "Le DataFrame final contient 475,911 lignes et 69 colonnes.\n",
      "------------------------------------------------------------\n",
      "Le DataFrame est maintenant stocké dans la variable 'df'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Identifiant de l'accident</th>\n",
       "      <th>Date et heure</th>\n",
       "      <th>Commune</th>\n",
       "      <th>Année</th>\n",
       "      <th>Mois</th>\n",
       "      <th>Jour</th>\n",
       "      <th>Heure minute</th>\n",
       "      <th>Lumière</th>\n",
       "      <th>Localisation</th>\n",
       "      <th>Intersection</th>\n",
       "      <th>...</th>\n",
       "      <th>year_georef</th>\n",
       "      <th>Nom Officiel Commune</th>\n",
       "      <th>Code Officiel Département</th>\n",
       "      <th>Nom Officiel Département</th>\n",
       "      <th>Code Officiel EPCI</th>\n",
       "      <th>Nom Officiel EPCI</th>\n",
       "      <th>Code Officiel Région</th>\n",
       "      <th>Nom Officiel Région</th>\n",
       "      <th>Nom Officiel Commune / Arrondissement Municipal</th>\n",
       "      <th>Code Officiel Commune</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201700033619</td>\n",
       "      <td>2017-05-19T17:40:00+02:00</td>\n",
       "      <td>Perpignan</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>17:40</td>\n",
       "      <td>Plein jour</td>\n",
       "      <td>Hors agglomération</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Perpignan</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Pyrénées-Orientales</td>\n",
       "      <td>200027183.0</td>\n",
       "      <td>CU Perpignan Méditerranée Métropole</td>\n",
       "      <td>76.0</td>\n",
       "      <td>Occitanie</td>\n",
       "      <td>Perpignan</td>\n",
       "      <td>66136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201700048262</td>\n",
       "      <td>2017-12-25T17:55:00+01:00</td>\n",
       "      <td>Saint-denis</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>17:55</td>\n",
       "      <td>Crépuscule ou aube</td>\n",
       "      <td>Hors agglomération</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Saint-Denis</td>\n",
       "      <td>93.0</td>\n",
       "      <td>Seine-Saint-Denis</td>\n",
       "      <td>200054781.0</td>\n",
       "      <td>Métropole du Grand Paris</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Île-de-France</td>\n",
       "      <td>Saint-Denis</td>\n",
       "      <td>93066.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201700048288</td>\n",
       "      <td>2017-01-01T17:25:00+01:00</td>\n",
       "      <td>Gennevilliers</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17:25</td>\n",
       "      <td>Nuit sans éclairage public</td>\n",
       "      <td>Hors agglomération</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Gennevilliers</td>\n",
       "      <td>92.0</td>\n",
       "      <td>Hauts-de-Seine</td>\n",
       "      <td>200054781.0</td>\n",
       "      <td>Métropole du Grand Paris</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Île-de-France</td>\n",
       "      <td>Gennevilliers</td>\n",
       "      <td>92036.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201700048486</td>\n",
       "      <td>2017-05-04T00:04:00+02:00</td>\n",
       "      <td>Fontenay-le-fleury</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>00:04</td>\n",
       "      <td>Nuit avec éclairage public non allumé</td>\n",
       "      <td>Hors agglomération</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Fontenay-le-Fleury</td>\n",
       "      <td>78.0</td>\n",
       "      <td>Yvelines</td>\n",
       "      <td>247800584.0</td>\n",
       "      <td>CA Versailles Grand Parc (C.A.V.G.P.)</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Île-de-France</td>\n",
       "      <td>Fontenay-le-Fleury</td>\n",
       "      <td>78242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201700048639</td>\n",
       "      <td>2017-07-11T08:10:00+02:00</td>\n",
       "      <td>Velizy-villacoublay</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>08:10</td>\n",
       "      <td>Plein jour</td>\n",
       "      <td>Hors agglomération</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Vélizy-Villacoublay</td>\n",
       "      <td>78.0</td>\n",
       "      <td>Yvelines</td>\n",
       "      <td>247800584.0</td>\n",
       "      <td>CA Versailles Grand Parc (C.A.V.G.P.)</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Île-de-France</td>\n",
       "      <td>Vélizy-Villacoublay</td>\n",
       "      <td>78640.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Identifiant de l'accident              Date et heure              Commune  \\\n",
       "0               201700033619  2017-05-19T17:40:00+02:00            Perpignan   \n",
       "1               201700048262  2017-12-25T17:55:00+01:00          Saint-denis   \n",
       "2               201700048288  2017-01-01T17:25:00+01:00        Gennevilliers   \n",
       "3               201700048486  2017-05-04T00:04:00+02:00   Fontenay-le-fleury   \n",
       "4               201700048639  2017-07-11T08:10:00+02:00  Velizy-villacoublay   \n",
       "\n",
       "   Année  Mois  Jour Heure minute                                Lumière  \\\n",
       "0   2017     5    19        17:40                             Plein jour   \n",
       "1   2017    12    25        17:55                     Crépuscule ou aube   \n",
       "2   2017     1     1        17:25             Nuit sans éclairage public   \n",
       "3   2017     5     4        00:04  Nuit avec éclairage public non allumé   \n",
       "4   2017     7    11        08:10                             Plein jour   \n",
       "\n",
       "         Localisation  Intersection  ... year_georef Nom Officiel Commune  \\\n",
       "0  Hors agglomération             6  ...        2017            Perpignan   \n",
       "1  Hors agglomération             1  ...        2017          Saint-Denis   \n",
       "2  Hors agglomération             1  ...        2017        Gennevilliers   \n",
       "3  Hors agglomération             1  ...        2017   Fontenay-le-Fleury   \n",
       "4  Hors agglomération             1  ...        2017  Vélizy-Villacoublay   \n",
       "\n",
       "  Code Officiel Département Nom Officiel Département  Code Officiel EPCI  \\\n",
       "0                      66.0      Pyrénées-Orientales         200027183.0   \n",
       "1                      93.0        Seine-Saint-Denis         200054781.0   \n",
       "2                      92.0           Hauts-de-Seine         200054781.0   \n",
       "3                      78.0                 Yvelines         247800584.0   \n",
       "4                      78.0                 Yvelines         247800584.0   \n",
       "\n",
       "                       Nom Officiel EPCI Code Officiel Région  \\\n",
       "0    CU Perpignan Méditerranée Métropole                 76.0   \n",
       "1               Métropole du Grand Paris                 11.0   \n",
       "2               Métropole du Grand Paris                 11.0   \n",
       "3  CA Versailles Grand Parc (C.A.V.G.P.)                 11.0   \n",
       "4  CA Versailles Grand Parc (C.A.V.G.P.)                 11.0   \n",
       "\n",
       "  Nom Officiel Région  Nom Officiel Commune / Arrondissement Municipal  \\\n",
       "0           Occitanie                                        Perpignan   \n",
       "1       Île-de-France                                      Saint-Denis   \n",
       "2       Île-de-France                                    Gennevilliers   \n",
       "3       Île-de-France                               Fontenay-le-Fleury   \n",
       "4       Île-de-France                              Vélizy-Villacoublay   \n",
       "\n",
       "   Code Officiel Commune  \n",
       "0                66136.0  \n",
       "1                93066.0  \n",
       "2                92036.0  \n",
       "3                78242.0  \n",
       "4                78640.0  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# URL corrigée avec \"corporels\" au lieu de \"corpels\"\n",
    "url_csv = \"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents-corporels-de-la-circulation-millesime/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "\n",
    "print(\"Lancement du téléchargement avec l'URL corrigée. Veuillez patienter...\")\n",
    "\n",
    "try:\n",
    "    # 1. Effectuer la requête pour obtenir les données\n",
    "    response = requests.get(url_csv)\n",
    "    # Lève une exception (comme 404 Not Found ou 500 Server Error) si la requête échoue\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # 2. Lire le contenu du CSV\n",
    "    csv_data = io.StringIO(response.text)\n",
    "\n",
    "    # 3. Charger les données dans un DataFrame\n",
    "    df_complet = pd.read_csv(csv_data, sep=';')\n",
    "\n",
    "    print(\"Téléchargement et chargement dans le DataFrame terminés avec succès !\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # 4. Afficher le nombre de lignes et de colonnes\n",
    "    nombre_lignes = df_complet.shape[0]\n",
    "    nombre_colonnes = df_complet.shape[1]\n",
    "    \n",
    "    print(f\"Le DataFrame final contient {nombre_lignes:,} lignes et {nombre_colonnes} colonnes.\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Renommer le DataFrame en 'df' pour la suite du projet, c'est plus simple\n",
    "    df = df_complet.copy()\n",
    "    \n",
    "    print(\"Le DataFrame est maintenant stocké dans la variable 'df'.\")\n",
    "    display(df.head())\n",
    "\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Une erreur de connexion est survenue : {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Une erreur inattendue est survenue : {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
